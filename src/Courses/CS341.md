## Asymptotic Notation

### Math Notation

Review of asymptotic notation; CS341 assumes that we only care about very large inputs. 

We have: $O, \Omega, \Theta, o, \omega$. 

In terms of notation, we have that $T(n)$ is the runtime of a function with input $n$. 

O: $T(n) = O(f(n))$. That is, as input sized get very large, $T(n)$ is bonded above $c\cdot f(n)$.
Formally, $T(n) = O(f(n))$ iff there exists two postiive constants $c_1, n_0 > 0$, such that $\forall n \geq n_0, T(N) \leq c\cdot f(n)$.

Our job is to come up with $c_1, n_0$.

Example: $T(n) = 2n^2 + 8n - 10$.

Solution: We claim that $T(n) = O(n^2)$.

We have that:
$$
\begin{aligned}
    2n^2 + 8n - 10 \leq 2n^2 + 8n \leq 10n^2
\end{aligned}
$$
Thus, we have $c=10, n_0 = 1$.


Example: $T(n) = a_kn^k + \ldots + a_0n^0$.

We claim that $T(n) = O(n^k)$.

$$
\begin{aligned}
T(n) &\in |a_k|n^k + |a_{k-1}|n^{k-1}+\ldots a_0n^0\\
&\leq |a_k|n^k + |a_{k-1}|n^{k}+\ldots a_0n^k\\
&= n^k(|a_k| + |a_{k-1}| + \ldots + |a_0|)
\end{aligned}
$$

So we have $c= (|a_k| + |a_{k-1}| + \ldots + |a_0|), n_0 = 1$.

Possible concern: I can choose $c$ as large as I want and always make $T(n) \leq c*f(n)$.

Counterexample: $a_kn^k$ is not $O(n^{k-1})$.

Suppose for the purpose of contradiction that $a_kn^k \in O(n^{k-1})$. Then the math works out poorly and $n$ goes to infinity.

$\Omega$: $T(n) = \Omega(f(n))$ iff there exists $n_0, c > 0$ such that $\forall n\geq n_0, T(n) \geq c\cdot f(n)$.

Example: $T(n) = a_kn^k + \ldots + a_0n^0$. We claim this is $\Omega(n^k)$.

We will guess $c=\frac{a_k}{2}, n_0=?$.

We want $a_kn^k + \ldots + a_0n^0 \geq \frac{a_kn^k}{2}$ for all $n \geq n_0$.

If that's true then we move the right side to the left, and then move everything except the first term to the right):
$$
\begin{aligned}
    \frac{a_k}{2}n^k &\geq -a_{k-1}n^{k-1} - \ldots - a_0n^0\\
    &\geq -a_{k-1} - \frac{a_{k-2}}{n} - \ldots - \frac{a_0}{n^{k-1}}\\
  n &\geq \frac{-2a_{k-1}}{a^k} - \frac{2a_{k-2}}{a_kn} - \ldots - \frac{2a_0}{a_kn^{k-1}}
\end{aligned}
$$

We pick an $n_0 > k$. So $n_0 = |\frac{-2a_{k-1}}{a^k}| - |\frac{2a_{k-2}}{a_kn}| - \ldots - |\frac{2a_0}{a_kn^{k-1}}|$

Thus we have that $T(n) \geq \frac{a_k}{2}n^k$ which means $T(n) \in \Omega(n^k)$.

$\Theta$: $T(n) = \Theta(f(n))$ iff both Big O and Big Omega hold for $f(n)$.

Example $T(n) = \frac{3n^2 + 3n}{2}$. We show that it is $O(n^2)$ and $\Theta(n)$ and thus $\Theta(n^2)$.

We show that the lower bound first:

$$
\begin{aligned}
    \frac{3n^2 + 3n}{2} &\leq c\cdot n^2\\
\end{aligned}
$$

We can just let $c=5n$ etc.

o: $T(n) = o(f(n))$ iff $\forall c$, $\exists n_0 > 0$ such that $\forall n\geq n_0, T(n) \leq c\cdot f(n)$.

Our job is to show, for an arbitrary $c$, that $\exists n_0>0$ that satisfy the inequality. 

Example: $3n + 2  = o(n^2)$.

$$
\begin{aligned}
    3n+2 &\leq 5n \leq cn^2\\
    5 &\leq cn\\
    \frac{5}{c} \leq n
\end{aligned}
$$

So for all $c$, we choose $\geq max(1,\frac{5}{c})$.

$\omega: T(n) = \omega(f(n))$ such that $\forall c > 0, \exists n_0 > 0$ such that $\forall n \geq n_0, T(n) \geq cf(n)$.

Exercise: Show $n^2 \in \omega(3n+2)$.

### Proofs using Limit Law

$T(n) = o(f(n))$ iff $\lim_{n\to\infty}\frac{T(n)}{f(n)} = 0$

$T(n) = \omega(f(n))$ iff $\lim_{n\to\infty}\frac{T(n)}{f(n)} = \infty$

$T(n) = \Theta(f(n))$ iff $\lim_{n\to\infty}\frac{T(n)}{f(n)}$ is constant.

Example: Try $n\log n$ vs $n^2$.

### Exercises

$2^n$ vs $n^2$.

$$
\begin{aligned}
    \lim_{n\to\infty} \frac{2^n}{n^2} = \frac{2^n\ln2 n}{2n} = \frac{2^n\ln2}{2} = \infty
\end{aligned}
$$

$1.01^n$ vs $n^{100}$

$$
\begin{aligned}
    \lim_{n\to\infty} \frac{1.01^n}{n^100} = \ldots = \infty
\end{aligned}
$$

$\log^{100} n$ vs $n^{0.001}$ is little o. Do this as exercise.

$n^5 + 2^n$ vs $(n^2 + \frac{3^n}{1000}$ is little o.

$\log_5n$ vs $\log_6n$ is $\Theta$.

$2^{\log_5n}$ vs $2^{\log_6n}$ is little o.

Proof:
We use change of base such that we have
$$
\begin{aligned}
    2^{\log_5n} = 2^{\frac{\log_2n}{\log_5n}} = (2^{logn})^\frac{1}{\log_5n} = n^\frac{1}{\log_5n}
\end{aligned}
$$
and 
$$
\begin{aligned}
    2^{\log_6n} = 2^{\frac{\log_2n}{\log_6n}} = (2^{logn})^\frac{1}{\log_6n} = n^\frac{1}{\log_6n}
\end{aligned}
$$
We note that $\frac{1}{\log_5n}$ is a bigger degree.

### Important Summations

This is unfinished.

$$
\begin{aligned}
    \sum_{i=1}^n i &= \frac{n(n+1)}{2} = \Theta(n^2)\\
    \sum_{i=1}^ni^2 &= \frac{n(n+1)(2n+1)}{6} = \Theta(n^3)\\
    \sum_{i=1}^n i^2 &= \Theta(n^{2+1})
\end{aligned}
$$

## Reductions
Solving a computational problem $C_1$ by using an algorithm that solves aother computational problem $C_2$.

Suppose we have an instance of a computational problem $\Pi_{C_1}$. Then this output is converted to an instance of problem $\Pi_{C_2}$. Then, this is put to an algorithm for $C_2$ which gives the solution $S_2$. This solution is then given to a converter which helps us to solve $C_1$.

Consider the problem \verb|2-SUM|. 

The input: 2 integer arrays $A,B$ of length $n$ and another integer $m$.

The output: YES if and only if $\exists i,j : A[i] + B[j] = m$.

Suppose $\verb|A={7,12,4,26,15,1]}|$ and $\verb|B={4,15,6,0,-3,11]}|$ and $m=13$. Then answer is YES because $7+6=13$.

Naive algorithm: Check for all $i,j$ if $A[i] +B[j] = m$. This has quadratic runtime.

Better algorithm: We sort $B$ in $n\log n$ time. Then we go through all of $A$ and for each element in $O(n)$, we binary search in $O(\log n)$ in $B$ for the difference. If binary search works, we return YES. This has total runtime $n\log n$.

Now consider the problem $\verb|2-SORTEDSUM|$.

The input: $A,B$ are two sorted integer arrays of length $n$ and an integer $m$.

The output: YES if and only if $\exists i,j : A[i] + B[j] = m$.

```C++
i=1, j = n;
while (i <= n && j >= 1) {
    if (A[i] +B[j] == m) {
        return YES;
    } else if (A[i]+B[j] > m) {
        j--;
    } else {
        i++;
    }
}
```

Intuitively, we our guaranteed to hit every combination that is valid because if one side of the array hits a possible number, the other side will move downwards until it eventually reaches the right number (since its sorted too).

Formal Proof of Correctness: $\textbf{we do an IFF argument here}$

If the answer is NO, $\verb|2-SORTEDSUM|$ cannot find $A[i]+B[j]=m$.

If the answer is YES, then there is an $i*, j* : A[i*]+B[j*]=m$. \verb|2-SORTEDSUM| will eventually hit either $i*$ or $j*$. Suppose w.l.o.g, it hits $i*$ before it hits $j*$. At that moment $j > j*$. Which means $B[j] > B[j*]$ so $A[i] + B[j*] > m$ so \verb|2-SORTEDSUM| will keep running the algorithm on $j$ until it hits $j*$ at which point it will return YES.

### Reduction Example

To solve \verb|2-SUM|:
```C++
sort A,B;
return 2-SORTEDSUM(A,B);
```
Boom.

Now consider $$\verb|3-SUM|$$.

The input: $A,B,C$ of unsorted arrays of length $n$ and an integer $m$.

The output: YES iff $\exists i,j,k : A[i] + B[j] + C[k] = m$.

Naive algorithm: obvious.

Better algorithm: Using a reduction to \verb|2-SUM|.

```C++
for (int i = 0; i < n; i++) {
    if (2-SUM(B, C, M-A[i]) == YES) {
        return YES;
    }
    return NO;
}
```

We run an $n\log n$ algorithm $n$ times and so this algorithm runs in $O(n^2\log n)$.

Correctness follows from $2-SUM$ correctness.

Even faster algorithm:

```C++
Sort B, C;
for (int i = 0; i < n; i++) {
    if (2-SORTEDSUM(B,C, m-A[i])==YES) {
        return YES;
    }
    return NO;
}
```
This is $O(n^2)$.

## Recurrences
Recurrence is a math tool to analayze runtimes of divide and conquer algorithms. Our definition of divide and conquer? Anything that is kindof like MergeSort.

The template:

```C++
DCAlgorithm-Template(P){
Some base case;
solution1 = DCAlgorithm(subProblem1);
solution2 = DCAlgorithm(subProblem2);
...
solution_a = DCAlgorithm(subProblem_a);
combine(solution1, solution2,..., solution_a);
}
```

$\textbf{Recurrence}$: an equation or inequality tht describes a function $T(n)$ which is the runtime, in terms of $T$'s values on smaller inputs and a base case.

Example: MergeSort = $T(n)\leq 2T(\frac{n}{2}) + 7n$ where $T(2)=2$.

Example: $\verb|DCSearch|$. 

Input: An unordered array of $n$ numbers and an element $x$.

Output: YES iff $x$ is in the array.
```C++
DCSearch(A,x):
if (|A| == 1) return A[0] == x;
exists L = DCSearch(A[1...n/2],x);
exists R = DCSearch(A[n/2...n],x);
return exisits L OR exists R;
```
$T(n) = 2T(n/2) + 1$ and $T(1) = 1$.

Example: Hypothetical Divide and Conquer algorithm.

```C++
HDC(P of size n) {
    Base case if n==1 ...
    sol 1 = HDC (subProblem of size n/2);
    sol 2 = HDC (subProblem of size n/2);
    sol 3 = HDC (subProblem of size n/2);
    combine(sol1, sol2, sol3); // Combine takes n^2 work. 
}
```
$T(n) = 3T(n/2) + n^2$ and $T(1) = O(1)$.

### Solving Recurrences

$\textbf{Guess-and-Check}$ which is also called the Proof by Induction.

$\textbf{Recurrence Tree}$; see MergeSort.

$\textbf{Master Method}$ is easy to use but only when recurrence is in the following structure: $T(n) = aT(n/b) + O(n^d)$

We will start with a proof by induction. 

Example: Median-of-medians. $T(n) = \frac{T(n)}{5} + T(\frac{7n}{10} + n$ where $T(1)=1$.

Guess: $T(n)\leq10n$.

Proof: Base case $T(1) = 1 < 10\cdot 1 = 10$ as needed.

Inductive Hypothesis: $\forall k < n, T(k) \leq 10k$.

Prove: $T(n) \leq 10n$.
$$
\begin{aligned}
    T(n) &\leq T(\frac{n}{5}) + T(\frac{7n}{10}) + n\\
&\leq \frac{10n}{5} + \frac{10\cdot7n}{10} + n\\
&= 2n + 7n + n\\
&= 10n
\end{aligned}
$$
Where line 2 follows from the Inductive Hypothesis.

Another example of induction:

$\verb|DCSearch|$ = $T(n) = 2T(n/2) + 1, T(1)=1$.

Guess 1: $2n-1$

Base case: $T(1) = 1 \leq 1\cdot 2 - 1= 1$ as needed.

Inductive Hypothesis: $\forall k < n, T(k) \leq 2k$.

Prove: $T(n) \leq 2n$.

$$
\begin{aligned}
    T(n) &\leq 2n + 1\\
    &\leq 2(2(n/2) - 1) + 1\\
    &\leq 2n - 2 + 1\\
    &= 2n -1
\end{aligned}
$$

As needed.

Recall that
$$
\begin{aligned}
    \sum_{i=1}^k c^i = c + c^2 + \ldots + c^k
\end{aligned}
$$
is $\Theta(1) if c <1, \Theta(c^k) if c > 1, \Theta(k) if c==1$

### Recursion Tree Method
This is the most explicit and most intuitive method to see the total runtime of an algorithm. (Done on paper)

Example 2: $T(n) = 3(T(\frac{n}{2})  + n^2$

(Done on paper)

### Master Theorem

Observation: We saw hree uses of the recursion tree method demonstrating the three possibilities in the overall runtime analysis of a divide and conqer algorithm.

* Work decreases per level and work at root dominates (HDC)
* Work increases at each level and work in the leaves dominates (DCSearch). Runtime is number of leaves
* Work at each level stays the same (MergeSort) in which case total runtime was work at each level multiplied by the number of levels

Master Theorem is the third way of solving recurrences and turns this observation into a practical formula.

Master Theorem: Assume all subproblems have equal size.

Formally: $T(n)=aT(\frac{n}{b}) + O(n^d)$

$a$ = number of recursive calls, $b$ = input shrinkage factor, $d$ = exponent in the turntime of the combine step.

$$
\begin{aligned}
    T(n) &= O(n^d) \text{ if $a<b^d$}\\
    &= O(a^{\log_bn}=n^{\log_ba}) \text{ if $a > b^d$}\\
    &= O(n^d\log n) \text{ if $a=b^d$}
\end{aligned}
$$

Example: MergeSort: $a = 2, b = 2, d = 1$. Since $a = 2 = b^d = 2$, we use (3): MergeSort is in $O(n^1\log n)$

Example: DCSearch: $a=2,b=2,d=0$. Since $a = 2 > b^d = 2^0 = 1$, we use (2): DCSearch is in $O(2^{\log_2 n}) = O(n)$

Example: HDC: $a=3,b=2,d=2$. Since $a < b^d$ we use case 1.

## Divide and Conquer

### 2D Maxmima

Input: Set $P$ of $n$ 2D points.

Output: All maximal points.

A point, $p$, is maximal if no other point dominates it. This means there is no other $p'$ such that $p'.x > p.x$ and $p'.y > p.y$

Applications: Skyline queries, economics (optimal resource allocation), 

Naive algorithm: 
```C++
for each p in P
    for each p' in p
        check if p' dominates p;
    if no point dominates p:
    add(p);
```
This is $\Theta(n^2)$ algorithm.

Divide and Conquer Algorithm: 

Missing (check slides).

### Closest pair 

I missed this class


### Integer Multiplication
Input: 2 $n$-digit integers $X,Y$.\\
Output: $Z=XY$.

We will be working with base-10 integers but the algorithm and analysis works in base 2.

#### Primary School Algorithm
You place the numbers on top of each other and use the algorithm we learned in school:
```
    X =     2 3 4 5
    Y =   x 6 7 8 9
            --------
          2 1 1 0 5
        1 8 7 6 9
      1 6 4 1 5
  + 1 4 0 7 0
 -------------------
Z = 1 5 9 2 0 2 0 5
```
Multiplying 2 one-digit numbers is one operation. Addition of 2 one-digit numbers is one operation. Shift by 1 digit is one operation.

The runtime is in $\Theta(n^2)$. Note that just the shifts take $1+2+3+\ldots+n-1$ shifts which is quadratic.

Thought Experiment: A DC Algorithm divides $X,Y$ into integers of $\frac{n}{2}$ digits.

$X = a10^{\frac{n}{2}} + b\\
Y = c10^{\frac{n}{2}} + d$.

Example: \\$a=23, b=45\\c=67,d=89$

Notice that the shift by $\frac{n}{2}$ is cheaper than multiplying 2 $\frac{n}{2}$-digit integers.

Then, $X\times Y= (a10^{\frac{n}{2}} + b)(c10^{\frac{n}{2}} + d) = ac10^n + (ad+bc)10^\frac{n}{2} + bd$

The observation is that there are four multiplications of two $\frac{n}{2}$ digit integers above.
```C++
DC-Multi1 (int X, int Y, int n) {
    Base Case: ...;
    Define a,b,c,d as above;
    
    Vac = DC-Multi1(a,c);
    Vad = DC-Multi1(a,d);
    Vbc = DC-Multi1(b,c);
    Vbd = DC-Multi1(b,d);
    
    Combine:
    tmp1 = Vac * 10^n // shift by n so O(n)
    tmp2 = (Vad + Vbc); // O(n) because n/2 digits each
    tmp3 = tmp2 * 10^(n/2) // shift by n/2 in O(n)
    
    return tmp1 + tmp3 + Vbd; // O(n)
}
```
Runtime of combine: $O(n)$

$T(n) \leq 4T(\frac{n}{2}) + O(n)$.

$a=4, b=2, d=1 : O(n^{\log_2 4}) = O(n^2)$

This is no better than elementary school lol.

So lets consider the fact that we actually only care about three numbers: $ac, bd, ac+bc$. So we need to combine $ac+bc$ as one computation/number.

Observe that: $a\times c$, $b\times d$, $(a+b)(c+d) = ac + ad + bc + bd$.

This leads to the Karatsuba-Opman Algorithm.

#### Karatsuba-Opman Algorithm

Input: $X,Y$ $n$-digit integers.
```C++
Base Case: ....;

set a,b,c,d as before;

Vac = KO(a,c);
Vbd = KO(b,d);
tmp = KO(a+b, c+d);

return Vac10^n + (tmp - Vac - Vbd)10^(n/2) + Vbd); // O(n)
```

$T(n) \leq 3T(\frac{n}{2}) + O(n)$.

This gives $O(n^{\log_2 3}) = O(n^{1.59})$

We can generalize KO to divide into $k>2$ which gives $O(n^{\log_k(2k+1)} \in O(n^{1+\epsilon})$.

Best known: $O(n\log n\log \log n)$.

### Matrix Multiplication
Input: 2 $n\times n$ matrices $A,B$.
Output: $C= A\times B$, an $n\times n$ matrix.

$
\begin{pmatrix}
a_{11} & \cdots & a_{1n} \\
a_{21} & \cdots & a_{2n} \\
\cdots & \cdots & \cdots \\
a_{n1} & \cdots & a_{nn} \\
\end{pmatrix} \begin{pmatrix}
b_{11} & \cdots & b_{1n} \\
b_{21} & \cdots & b_{2n} \\
\cdots & \cdots & \cdots \\
b_{n1} & \cdots & b_{nn} \\
\end{pmatrix}
= \begin{pmatrix}
c_{11} & \cdots & c_{1n} \\
c_{21} & \cdots & c_{2n} \\
\cdots & \cdots & \cdots\\
c_{n1} & \cdots & c_{nn} \\
\end{pmatrix}
$

By definition, $c_{ij}= \sum_{k=1}^n a_{ik}b_{kj} = a_{i1}b_{1j} + \ldots + a_{in}b_{nj}$

The number of computations to compute one $c_{ij}$ takes $n$ multiplcations and $n-1$ additions which is $O(n)$.

The standard algorithm:
```C++
Algo(Matrix A, Matrix B) {
Matrix C = empty;

for (int i = 0; i < n; i++) {
    for (int j = 0; j < n; j++) {
        C[i][j] = 0;
        for (int k = 0; k < n; k++) {
            C[i][j] += A[i][k] * B[k][j];
        }
    }
}

return C;
}
```
This is $\Theta(n^3)$

Thought Experiment: A DC algorithm divides.

Fact: 
$\begin{pmatrix}
A_{11} & A_{12} \\
A_{21} & A_{22}
\end{pmatrix}\begin{pmatrix}
B_{11} & B_{12} \\
B_{21} & B_{22}
\end{pmatrix}=\begin{pmatrix}
C_{11} & C_{12} \\
C_{21} & C_{22}
\end{pmatrix}$
where $A_{ij}, B_{ij}, C_{ij}$ are matrices. Then, each sub-matrix behaves as an atmoic element.

Ex. $C_{ij} = A_{i1}B_{1j} \bigoplus A_{i2}B_{2j}$

Recal matrix addition:
$\begin{pmatrix}
a_{11} & \cdots & a_{1n} \\
a_{21} & \cdots & a_{2n} \\
\cdots & \cdots & \cdots\\
a_{n1} & \cdots & a_{nn} \\
\end{pmatrix} \bigoplus
\begin{pmatrix}
b_{11} & \cdots & b_{1n} \\
b_{21} & \cdots & b_{2n} \\
\cdots & \cdots & \cdots\\
b_{n1} & \cdots & b_{nn} \\
\end{pmatrix}=
\begin{pmatrix}
c_{11} & \cdots & c_{1n} \\
c_{21} & \cdots & c_{2n} \\
\cdots & \cdots & \cdots\\
c_{n1} & \cdots & c_{nn} \\
\end{pmatrix}
$
Where $c_{ij} = a_{ij} + b_{ij}$


Consider the following matrices:
$\left( \begin{array}{cccc}
2&4 & 3&2 \\
3&5 & 4&2\\
6&5 & 9&3 \\
2&2 & 3&5
\end{array} \right)
\left( \begin{array}{cccc}
3&2 & 7&2 \\
2&4 & 8&4\\
5&2 & 9&3 \\
3&4 & 2&2
\end{array} \right)=
\left( \begin{array}{cc}
C_{11} & C_{12}\\
C_{21} & C_{22}
\end{array} \right)$

$C_{11}= A_{11}B_{11} \bigoplus A_{12}B_{21}$

$\left( \begin{array}{cc}
2 & 4\\
3 & 5
\end{array} \right) \left( \begin{array}{cc}
3 & 2\\
2 & 4
\end{array} \right) \bigoplus \left( \begin{array}{cc}
3 & 2\\
4 & 2
\end{array} \right) \left( \begin{array}{cc}
5 & 2\\git 
3 & 4
\end{array} \right) = \left( \begin{array}{cc}
14 & 20\\
19 & 26
\end{array} \right) \left( \begin{array}{cc}
21 & 14\\
26 & 10
\end{array} \right) = \left( \begin{array}{cc}
35 & 34\\
45 & 42
\end{array} \right)$

Check $C_{11}=2\times 3 + 4\times 2 + 3\times 5 + 2\times 3 = 35$
```C++
DC-MM(Matrix A, Matrix B) {
Base Case: ....;

Divide Aij and Bij into 8 matrices as before.

C11 = Matrix-add(DC-MM(A11, B11), DC-mm(A12,B12));
C12 = Matrix-add(DC-MM(A11, B12), DC-mm(A12,B22));
C21 = Matrix-add(DC-MM(A21, B11), DC-mm(A22,B22));
C22 = Matrix-add(DC-MM(A22, B21), DC-mm(A22,B22));

return C = matrix [C11, C12]
                  [C21, C22]    
}
```
Runtime of combine is $4(\frac{n^2}{4}) = O(n^2)$

Runtime total is $T(n) \leq 8T(\frac{n}{2}) + O(n^2)$

Using master method: $\Theta(n^{\log_2 8}) = \Theta(n^3)$

There's a faster algorithm called Strassen's algorithm which is less than cubic runtime. 

## Greedy Algorithms

### Job Scheduling 1

**Input**: A set of $n$ jobs with only lengths.

**Output**: Scheduling minimizing $\sum_{i=1}^n C_i$.

**Algorithm**: Shortest Job first

Example: $J_1 = 3, J_2 = 5, J_3 = 1, J_4 = 1$ would give us $S_g = J_3, J_4, J_1, J_2$.

**Claim**: Greedy shortest first is optimal.

**Proof**: We will use a greedy stays ahead argument.

For any schedule $S$:

Let $S[i]$ be $i$'th job schedule by $S$. Let $S_g$ be the greedy schedule. Let $cost(S, j) = \sum_{i=1}^j C_i$.

Example: $Cost(s, 3) = C_{S[1]} + C_{S[2]} + C_{S[3]}$.

**Key Claim**: $cost(S_g, j) \leq cost(S, j)$ for any arbitraty schedule $S$ for any $j$.

**Proof by induction on $j$**:

**Base Case**: $j=1$ so $cost(S_g, 1) = S_g[1].length \leq cost(S, 1) = S[1].length$ because $S_g[1]$ is the shortest schedule.

**Inductive Hypothesis**: Suppose the claim holds for $j=k$, so $cost(S_g, k) \leq cost(S, k)$.

Prove the claim holds for $j=k+1$:

$cost(S_g, k+1) = cost(S_g,k) + C_{S_g[k+1]} = cost(S_g,k) + \sum_{i=1}^{k+1} S_g[i].length$

Or similarly, $\sum_{i=1}^k C_{S_g[i]} + \frac{C_{S_g[k+1]}}{j} = \sum_{i=1}^k C_{S_g[i]} + \sum_{i=1}^{k+1} S_g[i].length$.


Now,

$cost(S, k+1) = cost(S, k) + \sum_{i=1}^{k+1} S[i].length$.

Based on inductive hypothesis, we have that $cost(S_g,k) \leq cost(S, k)$.

We must now show that $\sum_{i=1}^{k+1} S_g[i].length \leq \sum_{i=1}^{k+1} S[i].length$.

By the greedy criterion of our algorithm ,$\sum_{i=1}^{k+1} S_g[i].length \leq \sum_{i=1}^{k+1} S[i].length$ because the greedy algorithm picks the *shortest* $k+1$ jobs. 

Thus we have that $cost(S_g,k+1) \leq cost(S, k +1)$ and by the power of induction, $S_g$ is indeed optimal.

Note that we're comparing the SUM of the first $k+1$ jobs not just the $k+1$'th job.

### Job Scheduling 2

**Input**: A set of $n$ jobs with lengths *and* weights.

$J_i: [length, weight]$

**Output** A schedule minimizing $\sum_{i=1}^n w_iC_i$ (the sum of the weighted completion time).

Example: 

$J_1 : [3,1], J_2 : [5,2], J_3 :[{1, 3}], J_4: [{1, 1}]$

One example to schedule this is: $J_4, J_3, J_1, J_2$ which has cost $1\cdot 1 + 1 \cdot 3 + 3 \cdot 1 + 5 \cdot 2 = 32$.

*Question*: What if the weights are the same? Then same as job scheduling 1 (shorter jobs first).

*Question*: What if the lengths are the same? The same as job scheduling 1 but we do higher weights first.

*Question*: What to do with mixed lengths and weights?

$J_1: [3,1], J_2: [5,2]$.

Intuition 1 says $J_1$ first. Intuition 2 says $J_2$ first because higher weight.

**General Strategy**: We want to combine length and weight into a single score that captures both intuitions and then sort based on that score.

*Importantly*: The combined score $f(l_i, w_i)$ should satisfy:

1. If weights are the same then shorter jobs have lower score
2. If lengths are the same then higher weight jobs should have lower score

**Possible Combined Scores**

1. $\frac{l_i}{w_i}$
1. $l_i - w_i$

Is either score incorrect?

1. $\frac{l_i}{w_i}$
    * $J_1$: 3
    * $J_2$: 2.5
    * Schedule: $J_2, J_1$
    * Cost: 2 x 5 + 1 x 8 = 18
2. $l_i - w_i$
    * $J_1$: 2
    * $J_2$: 3
    * Schedule: $J_1, J_2$
    * Cost: 1 x 3 + 2 x 8 = 19

Therefore, 2. is incorrect.

**Claim**: Greedy-sort by $\frac{l_i}{w_i}$ is optimal.

**Runtime**: $O(n\log n)$

Example from before:

$J_1: 3, J_2: 2.5, J_3: 1/3, J_4: 1$ so we have the schedule: $J_3, J_4, J_2, J_1$


**Proof of optimality** (By an exchange argument):

We will argue that any $S$ can be transformed step by step, by only getting better, into $S_g$.

*Let's rename the jobs* such that $S_g = J_1J_2J_3\ldots J_n$ (we are renaming the jobs such that $J_1$ is the lowest score)

Let $S$ be any other solution such that $S \not= S_g$. Need to prove that $cost(S_g) \leq cost(S)$

*Observe*: that in $S_1$ thre must be a job $J_s$ that comes right after a job $J_b$ such that $j < b$.

i.e
```
           Jb Js
S = J1J2...J9 J6...
```

Let's modify $S$ into $S'$ by "exchanging" $J_b$ and $J_g$.

**Question**: How does the cost of $S$ change after the exchange?

```
S = ......][lb, wb][ls,ws][.......]
S'= ......][ls, ws][lb,wb][.......]
```
Recall: cost of $S = \sum_{i=1}^n w_i C_i$.

What is the change in cost after this exchange?

1. Completion time of $J_b$ increases by $l_s$
   1. This increases the cost by $l_sw_b$
2. Completion time of $J_s$ decreases by $l_b$
   1. This decreases the cost by $l_bw_s$

$$
\begin{aligned}
\Delta Cost = l_sw_b - l_bw_s &\leq 0\\
l_sw_b &\leq l_bw_s\\
\frac{l_s}{w_s} &\leq \frac{l_b}{w_b}
\end{aligned}
$$

Therefore, if we keep exchangin out of order pairs:

1. We will eventually stop. Why? Because there are ${n\choose 2}$ possible swaps and once we exchange $J_i, J_k$ we never exchange them again. After at most ${n\choose 2}$ swaps we have to stop.
2. Moreoever, when we stop, because there are no out of order pairs, we end up with $S_g$. Since each swap only decreases (or keeps the same) the cost of $S_i$:
   $$ \begin{aligned}
    cost(S_g) \leq cost(S)
   \end{aligned} $$

Thus $S_g$ is optimal.

### Stable Matching

Consider the problem where:

**Input:** 

1. $n$ interns, $n$ companies
2. each intern/company has a prefernence list over company/interns

![](screenshots/stablematchinginpit.png)

Each company has a ranking of interns in the left matrix. Each intern has a ranking of companies in the right matrix. How do we "match" the middle diagram optimally?

**Output**: a matching $M$ between interns and companies that is *stable*

**Stable Matching Definition:** 

There is no pairs $(c,i)$ and $(c',i')$ such that:

for $c: i' > i$ AND for $i' :c > c'$

For example, $c$ and $i'$ would prefer each other to $i$ and $c'$.

Every intern and company must be matched (perfect matching).

##### Examples

**Example:**

![](screenshots/matchingexample1.png)

**Example:**

![](screenshots/matchingexample2.png)

**Example:**

Here we start the stable proof by looking at interns choices:

![](screenshots/matchingexample3.png)

**Example:**

![](screenshots/matchingexample4.png)

Thus we have the same stable matching for the same example as before.

##### Questions

1. Does a stable matching always exist for any preference list
   1. How can we know that for some lists, there is no stable pair?
2. If a stable matching always exists, can we find it efficiently?
3. If a stable matching always exists, is it unique? No, we just showed this.

##### Gale and Shapleys Greedy Algorithm (1962)

This is going to be very image heavy.

The idea is that we pick a company, choose an intern, and see if the intern can accept or not.

Suppose we start with every company and intern unmatched.

![](screenshots/greedymatch1.png)

We will pick any unmatched company. Let's choose 2.

![](screenshots/greedymatch2.png)

Company 2's top choice is intern $A$. Since A is free, it will accept. This can be considered a tentative contract.

We do the same thing with company 3.

![](screenshots/greedymatch3.png)

3 also wants $A$. A is not free BUT prefers 3 so A will leave 2 and work for 3.

Now we go back to 2 (we don't have to go back). We can pick any unmatched company who hasn't offered to every intern.

![](screenshots/greedymatch4.png)
![](screenshots/greedymatch5.png)
![](screenshots/greedymatch6.png)
![](screenshots/greedymatch7.png)
![](screenshots/greedymatch8.png)

Is this a stable matching?

Yes. Intern A has his top so it's good. Intern B is matched to his top so it's good. Intern C is matched to 3 which is the next best choice. Intern D is matched to 2 which is all that's left.

**Pseudocode:**
```C++
GS-Algorithm(list companies, list interns, matrix rankings) {
    // Initially unmatched
    while (unmatched companie exists that hasn't proposed to all interns) {
        c = an arbitrary company;
        i = highest ranked intern for c that hasn't been proposed to.

        c.proposeTo(i);

        if (i.isFree()) (c, i) is made a matching for now.
        else {
            c' = company i is matched to right now
            if (i.rank(c) > i.rank(c')) {
                (c, i) are matched and c' is unmatched
            }
        }
    }
}
```

**Will this terminate**? Yes, eventually each company will ask all $n$ interns. After this, we have nothing more to consider.

This gives runtime $O(n^2)$.

**Will it return a matching**?

Consider a free intern $i$ that at some point is matched to a company. 

Is $i$ ever unmatched again? No, because either we stick with our current company or CHANGE to a different company. We will never be free or jobless.

Also, a matched interns companies can only get better or worse.

**Observation 1: A matched intern $i$ remains matched forever and $i$'s companies get better and better**

Consider a free company $c$ that is matched to an intern.

Is $c$ ever unmatched again? Yes, if $c$'s match leaves for another company.

Also, $c$'s matches get worse and worse over time.

**Observation 2: At each match, $c$ proposes to an intern lower in $c$'s list**

Can there be an unmatched company in the end? Like so:

![](screenshots/unmatchedexample.png)

Not possible, $C$ would have accepted any offer coming his way. Thus the algorithm is not finished

**Does this return a stable matching?**

![](screenshots/stablematchingquestion.png)

The point is that, at some point an intern will encounter it's top choice.

Can there be an unstable pair $(c,i),(c',i')$?

Claim: No.

Proof by contradiction:

Suppose that $c$ prefers $i' > i$ and $i'$ prefers $c > c'$.

This means that at some point in time, $c$ proposed to $i'$ and $i'$ made two actions:

1. $i'$ rejected $c$ because $i'$ was matched to $c''$ which was higher ranked for $i'$. Then $i'$ ranks $c'' > c > c'$. Then we must have a contradiction because $i'$'s companies only get better and better.
2. Got matched to $c$ but left $c$ for another company $c''$. This is a contradiction as well.

**What have we proved so far?**

REGARDLESS of preference:

1. There is always a stable matching
2. We can find it efficiently in quadratic time.

Why? Because of this algorithm. Damn.

**Does the algorithm return the SAME stable matching?**

It seems that this algorithm favors companies. That is, companies tend to get their better choices.

At this point in lecture, we ran the algorithm but allowed interns to propose and companies to accept or reject.

![](screenshots/internspropose.png)

We see a second stable matching that seems to be better for the interns.

Let's try to prove the favouritism.

**Definition:** An intern $i$ is valid for $c$ iff $\exists$ a stable matchingin which $c$ is matched with $i$. And vice versa for interns.

**Definition:** An intern $i$ is optimal for $c$ iff $c$ ranks $i$ highest among all valid inerns for $c$.

**Definition:** An intern $i$ is pessimal for $c$ iff $c$ ranks $i$ lowest among all valid interns for $c$.

**Claim 1**: Every company $c$ is matched $c$'s optimal intern by GS-Algo.

**Claim 2**: Every intern $i$ is matched to $i$'s pessimal company by GS-Algo.

Let $optimal(c)$ be the optimal intern for comapny $c$.

If the claim is true then $optimal(c) \not= optimal(c')$ for any $c, c'$.

Proof of the claim:

Suppose a company $c$ does not get matched with $optimal(c)$.

Suppose company 1 proposed to $optimal(1) = D$ but $D$ rejected 1 for 3.

By definition of $optimal$ intern, $\exists$ a stable matching $S*$ with $(1,D)$ is matched.

Then $D$ prefers $3 > 1$. Also 3 prefers $D > C$ because 1 was the first company $c$ rejected by optimal(c)). 

But then we don't have a stable matching which is a contradiction.

![](screenshots/stablematchingcontradiction.png)

Thus, we have proven that every company gets matched with their optimal intern.

## Dynamic Programming

### Linear Independent Set

**Input:** An undirected line graph $G(V,E)$ and weights on the vertices.

![](screenshots/lineGraph1.png)

**Output:** The *max-weight independent set* of vertices in $G$. i.e. a subset $S$ of vertices $V$ such that any pair of vertices $v$ and $w \in S$ are not adjacent.

Example: $\{A,D\} \to 4, \{B,D\}\to8$


##### Naive Brute Force Algorithm

Search all $2^n$ subsets and return the max weight independent set. $\Theta(2^n)$.

##### Greedy Algorithm

Pick highest weight vertex $v$ remove its adjacant neighbours. Repeat.

This is wrong because it will not always return the max. Even in the example above it's wrong.

##### Divide and Conquer

Can be made to work but is slow. Left as an exercise for the reader (lol).

The issue is that we need to resolve conflicts at the boundaries.

##### DP algorithm

Let $S*$ be the optimal solution to $G_n$. 

We can reason about what $S*$ might look like in terms of optimal solutions to smaller subproblems. 

![](screenshots/DP1.png)

We will consider the last node $V_n$.

**Tautology:** Case 1: $V_n \in S*$ or Case 2: $V_n \not\in S*$.

Let $G_n = G$, 

$G_{n-1} = G - \{V_n\}$, 

$G_i = G_n - \{V_n, V_{n-1},\ldots, V_{i+1}\}$

Notation: The optimal solution to $G_i$ is $opt_i$.

**If we are in case 2:** The last node is not in the optimal solution. 

Claim: That is $S*$ is optimal solution to $G_{n-1}$

Proof: Assume for a contradiction that $\exists S' \in G_{n-1}$ where $S'$ is an independent set and $weight(S') > weight(S*)$. 

Since $S'$ is also an independent set for $G_n$, $S*$ cannot be the optimal solution to $G_n$.

**If we are in case 1:** The last node is in the optimal solution. 

Claim: Then $S* - V_n$ is $opt_{n-2}$.

Proof: We know $V_{n-1} \not\in S*$ because $S*$ has to be independent and since we had $v_n$ we couldn't have had $v_{n-1}$.

Thus, $S*-v_n$ is a feasible solution to $G_{n-2}$. i.e. only contains vertices from $v1$ to $v_{n-2}$ and they are independent.

Assume for contradiction, there is an IS in $G_{n-2}, S''$ such that $weight(S'') > weight(S* - v_n)$. Then $S''\cup\{v_n\}$ is an IS in $G_n$ with higher weight thn $S*$; contradicting that $S*$ is optimal. 

**What we've established:**
If $v_n \not\in S*$, then $S*$ is $opt_{n-1}$.

If $v_n \in S*$, then $S*-v_n$ is $opt_{n-1}$.

Semih says that if we get this far and can't write the algorithm, his heart will be broken.

**A possible recursive algorithm**:
```C++
Rec_LIS(Graph(V, E), weights) {
    Base Case: if (|V| == 1) ...;

    Soln1 = Rec-LIS(G_{n-1});
    Soln2 = Rec-LIS(G_{n-2}) UNION {vn};

    return max(soln1, soln2);
}
```

Runtime: $T(n) = T(n-1) + T(n-2) + O(1);

Exercise: this is $\Theta(2^n)$.

![](screenshots/recursionTreeDP.png)

We see the issue is that we recalculate the same thing often even though there's really only $n$ distinct calls for each $G_i$.

**Solution:** we can store our results in a table.

This is called *memoization*; simply store and cache the results of different recursive calls and check the cache before recursing. Exercise: show runtime becomes $\Theta(n)$.

In CS341, this is **not** accepted as dynamic programming.

**Bottom-up Iterative Formula**
```C++
DP-LIS(Graph(V, E), weights) {
    // A[i]: max weight IS in $G_i$
    Array a[n]; // Solution array of size n

    Base Cases: 
    initialize A[0] = 0.
    initilize A[1] = w1;

    for (i: 2 to n) {

        A[i] = max(A[i-1],A[i-2]+wi);
    }
    
    
    return A[n];

}
```

Correctness follows from the correctness of our recurrence.

Rnuntime is lnear

### Recipe of a DP Algorithm

1. Identify small subproblems. Represent solutions to subproblems inside an array.
2. Quickly + correctly solve larger subproblems given solutions to smaller ones (this is usually a recursive formula)
3. Computer final solution after finding all solve subproblems

### 0/1 Integer Weight Knapsack

**Input:** 

$n$ items: \{0,...,n\}
values for items $v_1,\ldots, v_n$ (integers)
capacity $W$.

**Output**:

Subset $S \subset \{O_1,\ldots,O_n\}$ with max value subject to $\sum_{i\in S} w_i \leq W$.

**Thought Experiment**:

Consider optimal solution $S*$.

**Tautology**: Case 1: $O_n \not\in S*$ or Case 2: $O_n \in S*$.

Claim 1: $S*$ is in case 1. Then, $S*$ is the optimal solution for $\{O_1,\ldots,O_{n-1}\}$ and capacity $W$. If $S*$ is in case 2, then $S*-\{O_n\}$ is optimal for $\{O_1,\ldots, O_{n-1}\}$ and capacity $W-w_n$.

**Subproblem:** Solution array $A$ will be two dimensional: $A[n][W] = max(A[n-1][W], A[n-1][W-w_n] + v_n)$. 

That is, $A[i][j]$

```C++
DP-KS({O1,...,On}, W) {
    A[n][W] = {0}; // A[i][c] is the max value to pick from {O_1,...,Oi} into a knapsack with capacity C

    for i=1...n
        for c=1...W
            A[i][c] = max(A[i-1][c], A[i-1][c-wi] + vi);
    return A[n][W];
}
```

### Matrix Multiplication Order

**Input**: Dimensions of $n$ rectangular matrices.

![](screenshots/matrixOrderInput.png)

**Output:** Find order of multiplying matrices with minimum cost using standard matrix multiplication

Min cost parenthesization for multiplying $A_1,\ldots, A_n$:

![](screenshots/matrixOrderOutput.png)

##### Example

![](screenshots/matrixExample1.png)

The cost of $(A\times B) = 100 * 5 * 20 = 10000$
![](screenshots/matrixExample2.png)

The cost of $(Y)\times C = 100\times20\times10 = 20000$
Total cost is $30000$.

Alternatively:

![](screenshots/matrixExample3.png)

**Fact**: There are $\frac{4^n}{n^{3/2}}$ different parenthesis

**Notation**: Let $A_{i\ldots j}$ be the result of multiplying $A_i, A_{i+1},\ldots, A_j$. Let $OPT_{i\ldots j}$ be the min-cost parenthesization for computing $A_{i\ldots j}$

##### Thought Experiment

Consider $OPT_{1\ldots n}$ and focus on its last computation. 

**Tautology:** 

Case 1: $A_{1\ldots1}\times A_{2\ldots n}$.

Case k: $A_{1\ldots k}\times A_{k+1\ldots n}$

Case $n-1$: $A_{1\ldots n-1}\times A_{n\ldots n}$

**Claim**: Suppose $OPT_{1\ldots n}$ is in case $k$. i.e. $OPT_{1\ldots n} = (A_1\ldots k)\times(A_{k+1\ldots n})$.

Then $OPT_{1\ldots n}$'s parentehszaition must be using $OPT_{1\ldots k}$ and $OPT_{k+1\ldots n}$.

Therefore, 

$OPT_{i\ldots j} = min(\ldots, OPT_{i\dots k} + OPT_{k+1\ldots j} + d_{i-1}d_kd_j)$

The number of subproblems is given by $n\choose 2$ because for each $(i,j)$ pair there will be a subproblem.

```C++
A[n][n] // A[i][j] be will min-cost parenthesization for multiplying A_i .. A_j = OPT_{i...j}

A[i][i] = 0;

A[i][i+1] = d_{i-1} * d_{i} * d_{i+1};

for (int i = 1; i < n; i++) {
    for (int j = 1; j < n;j++) {
        min_ij = infinity;
        for (int k = i; i < j-1) {
            min_ij = min(min_ij, A[i][k]+A[k+1][j]+d_{i-1}*d_i *d_{i+1});
        }
        S[i][j] = min_ij;
    }
}
return S[1][n];
```

THIS IS WRONG. When we do $A[k+1]$ we might access non-computed values yet.

Make sure you verify that your loop structure is correct. i.e. when solving a larger subproblem, $P$ all of the smaller subproblems $P$  depend on having been solved.

Solution: We can traverse diagonally.

## Graphs

### Introduction

A graph $G(V,E)$.

$V$: a set of nodes/vertices.

$E$: a set $(u,v)$ of edges such that $u,v$ are $\in V$.

Example:

![](screenshots/exampleGraph.png)

They are the natural way to represent connected data (consisting of a set of entities and relationships between them).

#### Examples

Facebook and Twitter. In facebook, each node is a person and an edge represents a frienship. In twitter, each node is a person and edge represents following.

The internet. Also maps. Contrast this with tables, spreadsheets.


#### Terminology

* Directed vs Undirected: Directed means edges have direction. Facebook undirected, Twitter directed.
* Simple vs Multigraph: Simple graph has at most on edge between two nodes. Multigraph can have many.
* Cyclic vs Acyclic graphs: Cyclic graph as a cycle (recall MATH239).
* Connected vs Unconnected: Connected means the graph visually is in one piece (for undirected).

#### Conventions

$|V|=n$ is the number of vertices.

$|E|=m$ is the number of edges.

Question: In an undirected graph $G$ what is the maximum number of edges you can have.

Answer: $m\leq n\choose 2$.

Question: What if $G$ is directed.

Answer: $m\leq 2* {n\choose2}$.

Question: If $G$ is undirected, connected what is the minimum number of edges?

Answer: $m \geq n -1$.

So for us, when $G$ is connected, 

$\begin{aligned}
n-1 &\leq m &\leq O(n^2)\\
\log m & = \Theta(\log n)
\end{aligned}$

Convention is that we use $O(\log n)$ and not $O(\log m)$

For example Dijkstra's alogirt runs in $O(n\log n)$ and not $O(n\log m)$.

Definition: Degree of $v$:

$out-deg(v) =$ number of outgoing edges from $v$. #$(v,w)\in E$.
$in-deg(v) =$ number of incoming edges from $v$. #$(w,v)\in E$.

#### Graph Storage Format

1. Adjacency Matrix

![](screenshots/adjacencyMatrixExample.png)
![](screenshots/adjacenyMatrixExample2.png)

$(i,j) = 0$ if $(i,j)\not\in E$ otherwise 1.

Operations: 

Storage: $O(n^2)$
LookUp: Constant time.
Iterate over a vertex's out or in edges: Linear time.

2. Adjacency List Format

![](screenshots/adjacencyList.png)

It's a list of outgoing edge.

Storage:  $O(n+m)$.
Lookup: $O(deg(u))$.
Iterating over outer edges: $O(deg(u))$.

**IMPORTANT QUANTITY**

For a *directed* graph:
$\sum_{v=1}^n \text{out-deg}(v) = m = \sum \text{in-deg(v)}$.

For an *undirected* graph:
$\sum_{v=1}^n \text{out-deg}(v) = 2m = \sum \text{in-deg(v)}$.

### Breadth-First vs Depth-First

#### BFS

BFS: Start from $s$ and traverse the graph in *waves*. That is, search $s\to s$'s first degree numbers then second degree.

![](screenshots/BFS1.png)

Then we traverse $B$ because $A$ is done. All of B's neighbours have been explored. Move on. Now we traverse $C$.

![](screenshots/BFS2.png)

![](screenshots/BFS3.png)

Now visit $H$.

![](screenshots/BFS4.png).

Semih is SO proud of these slides.

BFS is used for Queues.

Runtime is $O(m+n)$ becuse each $n$ node and we noted earlier that $\sum_{v=1}^n \text{out-deg}(v) = m = \sum \text{in-deg(v)}$.

#### DFS

Everytime we find an unvisited neighbour we look for another.

It is often used with stacks.

Feel free to use DFS and BFS in algorithms because they're basically graph primitives.

#### Properties

1. Both linear time algorithms
2. A BFS/DFS starting from $s$ will reach all vertices $t$ such that $s$ has a path to $t$.

### BFS Tree of a Connected Graph

Define "parent of $v$" $p(v)$: vertex $u$ that was being traverses when $v$ was first seen/visited.

BFS Tree($V_T,E_T)$ is the graph such that $V_T = V$ and $E_T=\{\forall v: (p(v), v)\}$

Suppose we have the following BFS-Tree of a directeed graph.

![](screenshots/BFSTree.png)

### Unweighted Single Source Shortest Paths

**Input:** $G(V,E)$ directed or undirected with a source vertex $s$
**Output:**: $\forall v\in V$, shortest path and shortest $dist(s,v)$.

Example:

![](screenshots/shortestPath1.png)

![](screenshots/shortestPath2.png)

The solution is to run BFS where the level numbers are distances.

What if we want to get the paths?

We use the BFS-Tree. Remember that the BFS Tree only keeps the parent child relationships according to the traversal that we did.

![](screenshots/shortestPath3.png)

### Bipartiteness/2-coloring

**Input:** undirected $G(V,E)$
**Output:** True/False to the following question:

can $V$ be partitioned as $V1$ (red) and $V2$ (green) such that $\forall (u,v)\in E, u \in red, v \in green$.

![](screenshots/bipartiteExample.png)
![](screenshots/bipartiteExample2.png)

Suppose we do BFS from $s$ and assign $s$ w.l.o.g the color red.

Then if $G$ is 2-colorable, each level should alternate colours. 

![](screenshots/bipartiteAlgorithm.png)

This is $\Theta(n+m)$

```C++

Bipartite-Checking(Graph G(V,E)) {
    run BFS(G);
    G.evenLevels = red;
    G.oddLevels = green;

    if (Edge e in E s.t E.u.color == E.v.colour) {
        return false;
    }
    return true;
}
```

### Connected components in undirected graphs

**Input:** undirected $G(V,E)$

**Output:** $CC_1,CC_2,\ldots, CC_k$ where each $CC_i$ is a maximal subset of $V$ such that each $s,t \in CC_i$ has a path to each other and each $v\in V$ is part of one $CC_i$.

![](screenshots/connectedExample1.png)

Idea:

Run BFS/DFS on a vertex until it terminates. Then we remove all those vertices visited and run it again until $V$ is empty.

We use label propogation. That is, for every instance of BFS, label nodes $A,B,C,\ldots$ respective to the instance.

![](screenshots/connectedExample2.png)

### Topological Sort

Directed Acyclic Graphs (DAGs) are graphs that model dependency relationships. They are *directed* and *acyclic*.

![](screenshots/DAGExample.png)

**Source:** No incoming edges
**Sink**: No outgoing edges

![](screenshots/dagExample2.png)

Every dag has at least 1 source. 

![](screenshots/dagProperties.png)

**Input:** A $G(V,E)$ which is a DAG.
**Output:** Vertices in sorted order such that each vertext $v$ appears after its depedencies.

Formally, given an input DAG $G(V,E)$ order the nodes such that if $(u,v)\in E$, then $u$ appears before $v$.

![](screenshots/topological1.png)

We choose a source and remove.

![](screenshots/topological2.png)

This can be done in $O(n+m)$.

Another way to do it:

Run DFS and number based on finishing times. We label a number when DFS can't continue any further. Then we order based on decreasing finishing times.

![](screenshots/DFSTopological.png)

```C++
topologicalSort2(DAG G) {
    run DFS(G) and put each finished vertex into an array in reverse order;
}
```

Runtime is $O(n+m)$.

**Proof of correctness:**

In a DAG, if $u\to v$ exists, then $f(u) > g(v)$.

**Proof**

In the DFS, either $u$ or $v$ will be visited first.

Case 1: $u$ is visited first.

Then DFS will visit $v$ before it finishes $u$.

Thus $f(u) > f(v)$.

Case 2: $v$ is visited first.

Then DFS will visit all its descendants and then terminate.

That is, it has to terminate before it can even start $u$.

Thus, $f(u) > f(v)$.

**Extended proof**:

If $u â¤³ v$, then $f(u) > f(v)$. Prove this by induction.

### Strongly Connected Components in Directed Graphs

These are strongly connected components:

![](screenshots/strongConnected1.png)

Given a directed graph $G(V,E)$.

$u$ and $v$ are strongly connected iff $u \rightsquigarrow v$ AND $v \rightsquigarrow u$.

An SCC of $G$ is a set $C \subeq V$ s.t. 

1. $\forall (u,v) \in C, u$ and $v$ are strongly connected
2. $C$ is non-empty and maximal. That is, $\forall v' \in V-C, v'$ is not strongly connected to any vertex in $C$.

It seems that a graph consisting of strongly connected compoents is a DAG.

![](screenshots/twoTiered.png)

![](screenshots/twoTieredDag.png)

I'm falling asleep in this class I should've went to bed earlier last night. 

The key idea is that if we start at the sinks we can use the old algorithm.

Let finishing time of an $SCC_i$ in DFS traversal be the maximum finishing time of the vertices in $SCC_i$.

$f[SCC_i]=$ max over $v \in SCC_i f[v]$.

![](screenshots/finishingTimes.png)

Key Lemma: In $G^{SCC}$ if $SCC_i \to SCC_j$ then $f[SCC_i] > f[SCC_j]$.

Furthermore:

We want to identify sinks NOT sources. Right now we can only identify sources. So we will reverse the graph. 

![](screenshots/reverseGraph.png)

Algorithm:

```C++
kosarajusSCC(DAG G) {
    run1 = DFS(reverse(G));
    finishingTimes = run1.finishingTimes;

    DFS(G, finishingTimes.sort());
}```

**Proof of Key Lemma**:

In $G^{SCC}$ if $SCC_i \to SCC_j$ then $f[SCC_i] > f[SCC_j]$.

Let $u$ be the first vertex visited in the DFS among the nodes in $SCC_i$ or $SCC_j$.

Two cases: either $u$ is in $SCC_i$ or $u$ is in $SCC_j$.

Suppose $u$ is in $SCC_i$. Then, $f(u) > f(v)$ for all $v \in SCC_j$ because DFS will visit and finish all in $SCC_j$ before it backtracks and finishes $u$. Therefore, $f(SCC_i) \geq f(u) > f(SCC_j)$.

Suppose $u$ is in $SCC_j$. Then, because $SCC_j$ does not have a path to $SCC_i$ (directed), all nodes in $SCC_i$ will be finished after all nodes in $SCC_j$ are finished. Thus, $f(SCC_i) \geq f(u) > f(SCC_j)$.

### Minimum Spanning Trees

#### Graph Theory Part 1

Definition of a tree: A tree is an undirected graph that is:
1. Acyclic
2. Connected

We will now cover properties of trees.

1. **Breaking a tree lemma**: Removing an edge from a tree disconnected $T$.

Proof: Assume removing an edge $(u,v)$ does not disconnect $T$.

Then there is still a path from $u$ to $v$. Then, before removing $(u,v)$, there was a cycle. This contradicts that $T$ was a tree.

2. **Breaking a Cycle Lemma**.

Let $G(V,E)$ be a connected graph.
Let $c \subseteq E$ be a cycle in $G$.

Then reoving any edge $(u,v)\in C$ cannot disconnect $G$.

3. **Cycle Creation Lemma**

Adding any edge $(u,v)$ to a tree $T$ creates a cycle.

**Theorems:**

Every tree of $n$ vertices contains exactly $n-1$ edges.

Every $n-1$ acyclic set of edges is a tree. That is, they connect the $n$ nodes in $V$.

Every $n-1$ set of edges that connects $n$ vertices is a tree. That is, they are acyclic.