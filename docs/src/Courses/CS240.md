## Useful Stuff

![Useful notation](screenshots/useful_stuff.png)

![Useful sums](screenshots/useful_sums.png)

## Asymptotic Analysis
What is the difference between an ADT and a data structure?
1. ADT is the data as well as the operations to manipulate the data whereas the data structure is the actual representation of the data.
2. Example: a linked list is an ADT and a tree is the data structure we would use to $\textit{implement}$ it

We use asymptotic notation to compare growth rate behaviours of algorithms for large input sizes.
1. Big O notation gives an soft upper bound. Given a function $f(n)$ and a function $g(n)$, we say that $f(n) \in O(g(n))$ if there is a constant $c > 0$ and an initial value $n_0 > 0$ such that $\vert f(n) \leq c \vert g(n) \vert$ for all $n \geq n_0$. Basically need to find a $c$ and an $n_0$ such that $f(n)$ is always less than equal to $g(n)$. If we want a \textbf{hard} upper bound, we use little o notation: $f(n) \in o(g(n))$ is stronger than big O because it applies for all $c > 0$, with some $n_0 > 0$ that $\vert f(n) \leq c \vert g(n) \vert$.

1. Big Omega ($\Omega$) gives us a soft lower bound and little omega ($\omega$) gives a hard lower bound.
1.  Theta notation is how we determine if a function $f$ is within both the upper and lower bound of $g$. $f(n) \in \Theta(g(n))$ implies that $c_1 g(n) \leq f(n) \leq c_2 g(n)$ for some $c_1, c_2, n \geq n_0$.

The limit rule allows us to find the relationship between $f$ and $g$. We take 
$$
\begin{equation}
L = \lim_{n\to\infty} \frac{f(n)}{g(n)}
\end{equation}
$$
Then it is quite intuitive to find the $\textbf{hard}$ bound. If $L = 0$, then $f(n)$ grows slower than $g(n)$ for all constants so we have by definition that $f(n) \in o(g(n)$. Likewise, if we have $L = \infty$, then the opposite is true and we have $f(n) \in \omega(g(n))$. Finally, if it's between $0$ and $\infty$, then we say there is a theta bound. Intuitively, this is because $f(n)$ and $g(n)$ must grow at similar growth rates, not including constants.

MergeSort is a divide-and-conquer algorithm for sorting an array. We split up the array into $k = 2$ subarrays and run MergeSort on them recursively. When we get down to $k$ elements, we sort manually. Then we merge the two halves (since left is sorted and right is sorted) in $\Theta(n)$ time. Here is the (general) pseudocode:
```C++
MergeSort(Array A) {
	if (A.size() == 0) {
    	return;
    }
    Array left = A.leftHalf;
    Array right = A.rightHalf;
    MergeSort(left);
    MergeSort(right);
    Merge(left, right);
}

Merge(Array left, Array right) {
    \\ left and right are both sorted arrays
    int i = 0;
    int j = 0;
    Array sorted[];
    while (i and j have valid indices) {
        if (left[i] < right[j]) {
    		sorted.push_back(left[i]);
        	i++;
    	} else {
        	sorted.push_back(right[j]);
            j++;
        }
    }
    while (i is less than left.size()) {
    	sorted.push_back(left[i]);
        i++;
    }
    while (j is less than right.size()) {
    	sorted.pushed_back(right[j]);
        j++;
    }
}
```
We call MergeSort on each half the array, giving runtime $2T(\frac{n}{2})$. And then we run Merge on each half, giving runtime $O(n)$. We know the each split will create a balanced binary tree whose height is $\log n$. Since we must go down the entire height of the tree running merge, we have an $\Theta (n)$ operation for each height giving us total runtime $\Theta(n \log n)$.

### Tutorial

##### Order Notation
Prove that $\log(n!) \in \Theta(n\log n)$:

a) We need to show that for $f(n) = \log(n!), g(n) = n\log n$, we have two constants $c_1, c_2$ such that for all $n \geq n_0$:
$$
\begin{equation}
c_1 n \log n \leq \log(n!) \leq c_2 n\log n
\end{equation}
$$
We will start by proving the upper bound. Note that: 
$$
\begin{align*}
\log(n!) &= \log(n \cdot (n-1) \cdot \cdots 1)\\
&= \log(n) + \log(n-1) + \cdots + \log(1)\\
&= \sum_{i=1}^n \log i\\
&\leq \sum_{i=1}^n \log n\\
&\leq n\log n
\end{align*}
$$
So we let $c_1 = 1, n_0 = 2$, and we have that $\log(n!) \in O(n\log n)$.

To prove the lower bound, it's a bit more complicated:
$$
\begin{align*}
\log(n!) &= \sum_{i=1}^n \log i \geq \sum_{i=\frac{n}{2}}^n \log i \geq \sum_{i=1}^{\frac{n}{2}} \log \frac{n}{2}\\
&\geq \frac{n}{2} \log \frac{n}{2}\\
&\geq \frac{n}{2} (\log (n) + \log (1/2))\\
&\geq \frac{n}{2} (\log n - \log 2)\\
&\geq \frac{n}{2} (\log n - 1)\\
&\geq \frac{n}{2} \left( \frac{1}{2} (\log n + \log n) - 1 \right)\\
&\geq \frac{n}{2} \left( \frac{1}{2} (\log n) + \frac{1}{2} (\log n) - 1 \right)\\
\end{align*}
$$
When $n \geq 4, \frac{1}{2} (\log n) - 1 \geq 0$, so:
$$
\begin{align*}
\log (n!) &\geq \frac{n}{2} \left( \frac{1}{2} (\log n) + \frac{1}{2} (\log n) - 1 \right)\\
&\geq \frac{n}{2} (\frac{1}{2} (\log n))\\
&\geq \frac{1}{4} (n\log n)
\end{align*}
$$
Thus we let $c_2 = 1, n_0 = 4$ to show that $\log(n!) \in \Omega(n\log n)$.

Prove that $2^n \in \omega(n^50)$

b) We want to prove that $2^n \in \omega(n^{50})$. That is, for all constants $c$ and all $n \geq n_0$, we have that:
$$
\begin{align*}
    c n^{50} \leq 2^n
\end{align*}
$$
Working backwards we see that:
$$
\begin{align*}
    c n^{50} &\leq 2^n\\
    \log(cn^{50}) &\leq n\\
    \log c + 50\log n &\leq n\\
    \log c + 50\log n &\leq \frac{n}{2} + \frac{n}{2}
\end{align*}
$$
We must have that $\frac{n}{2} \geq \log c$ or $\frac{n}{2} \geq 50\log n$. So either:
$$
\begin{equation}
    n \geq 100\log n
\end{equation}
$$
or
$$
\begin{equation}
    n \geq 2\log c
\end{equation}
$$
Thus we pick $n_0 = max(100\log n, 2\log c)$.
##### Proof/Disproof

If $f(n) \in O(g(n))$ and $g(n) \in \Omega(h(n))$ then $f(n) \in \Omega(h(n))$.

So we have that:
$$
\begin{align*}
    f(n) &\leq c_1 g(n)\\
    c_2 h(n) &\leq g(n)
\end{align*}
$$
And we're being asked about the soundness of:
$$
\begin{align*}
    c_3 h(n) \leq f(n)
\end{align*}
$$
Disproof: let $f(n) = n^2, g(n) = n^3, h(n) = n^3$. Then, we have that $f(n) \in O(g(n))$ and $g(n) \in \Omega(h(n))$, but clearly $f(n) \not\in \Omega(h(n))$. It suffices to prove that $f(n) \in o(h(n))$:

$$
\begin{align*}
    L &= \lim_{n\to\infty} \frac{n^2}{n^3}\\
    &= \lim_{n\to\infty} \frac{1}{n}\\
    &= 0
\end{align*}
$$

Thus, by the limit law, we have that $f(n) \in o(h(n))$ as needed.
### Loop Analysis

Analyze the following piece of pseudocode and give a tight ($\Theta$) bound on the
running time as a function of $n$.

```C++
int k = 1;
for (int i = 1; i <= n; i++) {
    int j = 0;
    while (j <= n) {
        j += k;
    }
    k *= 2;
}
```

Taking a look at the inner loop we notice that the value of $j$ is the same as $ik$ for a given iteration $i$. Then we have that the condition $\verb|j <= n|$ is equivalent to $\verb|ik <= n|$. That is, we now know that the inner loop occurs $\frac{n}{k}$ times. It remains for us to find the value of $k$. Taking a look at the outer loop, we see that the value of $k$ doubles each time such that it can be given by $k= 2^i$ where $i$ is the $i$'th iteration of the outer loop. Then for our inner loop we have that the runtime is $\frac{n}{2^i}$. Since our outer loop occurs from $1$ until $n$ we have that the total runtime $T(n)$ is:
$$
\begin{align*}
    T(n) &= \sum_{i=1}^n \frac{n}{2^i}\\
    &= n (\frac{1}{2})^i\\
    &\in \Theta(n)
\end{align*}
$$

## Priority Queues
Priority Queue is an ADT that is a collection of items with an operation to insert an item with a corresponding priority and a deleteMax which deletes and returns the item of highest priority. Consider it like a stack that pushes with a priority. Often used for queues, to-do lists, etc. You could even use it sort by pushing values from an array into the PQ (setting the value to be the priority) and then deleteMax to get the sorted array. More on this later.

 A binary heap is a type of tree with the following properties:
1. Every level that is not the last must be full.
1. The last level must be justified
1. For any node, $n$, in the tree, the parent is greater than or equal to $n$.

These properties allow us to make some assumptions about the height of the heap. We have that, for any height $h$, the minimum number of nodes is enough to fill the first level ($2^0 = 1$), the second level ($2^1 = 2$), up until the $h$'th level ($2^h$ nodes). We can similarly say that the maximum number of nodes is $2^{h+1}$. This property is really useful since we can easily say the height of a heap is $\Theta(\log n)$.

 We use an array to store a heap. Where the first node is at index 0 and the last is at index $n - 1$. The left child of a given node is $2i + 1$ and the right child is at $2i + 2$. We can "reverse-engineer" this to find the parent: $\frac{i-1}{2}$. Note that the array stores the tree in $\textbf{Level Order Traversal}$.

$\textit{Insertion}$: Insert at last index. Swap with parent while parent is smaller. Note that we have to do this at most $\log(n)$ times.

$\textit{Deletion}$: Delete the root (index 0). Put index $(n-1)$ at index 0. Swap down while parent is smaller than child (note we swap with the max of the children). Again this is at most a traversal of $\log n$.

Coming back to Priority Queues. We can then sort using one by implementing it as a binary heap. PQ-sort takes time $\Theta(n + n log n)$ but requires auxillary space since we copy the array into a data structure. Instead, we convert the input array into a heap via heapify:
```C++
heapify(Array A) {
    if (A.size() == 0) return;
    heapify(A.leftHalf);
    heapify(A.rightHalf);
    fix-down(A);
}
```
For this, we have that the recurrence relation is $\Theta(n)$ (as per the chart):
$$
\begin{equation}
T(n) =
\begin{cases}
	2(T(\frac{n}{2}) + \Theta(\log n) \text{ if $n > 1$}\\
    \Theta(1) \text{ otherwise}
\end{cases}
\end{equation}
$$

### Tutorial
#### Heap Insertion

Not gonna type this out but basically the way insertion works is: you insert at the next available leaf. Then you fix up on the inserted node until the parent is bigger than us.

##### Heapsort
Again not typing this up but basically you run heapify on the array (that is, we go through each "parent" and fix down on it). Then we simply deleteMax over and over. For each delete, we have to replace the root with the leaf and fix-down.
\subsubsection{Priority Updating}
Given an index $i$ in a heap of $n$ elements and a new priority value $p$, design an $O(\log n)$ algorithm that updates the priority of the element at index $i$ to the value $p$.

Since we have $i$, we're going to get the value and store it. Then we're going to delete the node at index $i$. That is, we replace it with the next available leaf and run fix-down and fix-up on it. This takes $\log n$ time. Then we insert into the tree the previous value but with priority $p$. Boom.

##### Multi-way Merge
Given a set of $k$ sorted arrays, where the combination of the $k$ arrays has $n$ elements in total, design an $O(nlogk)$ algorithm that produces a singlesorted array containing all $n$ elements. 

Since we don't know anything about the lengths of the sorted arrays, we can't use a modification of MergeSort for merging $k$ arrays. Furthermore, the question is asking for complexity $O(n\log k)$ which, if you look at heapsort, is only possible if the priority queue has, at most, $k$ elements. So the only way to do this is to create a priority queue and (if it exists) take the $i$'th element of each array. Since there are $k$ arrays, that means our priority queue has $k$ elements. Then, we deleteMin ONCE. Then, we take the next smallest element from the array that had the last deleted element. Rinse and repeat.

## Sorting and Randomized Algorithms

$\textit{Selection Problem: }$We want to find the $k$th largest element in an array. In a sorted array, this is simply the $k$th element.

The algorithm quickselect will do this for us but first we need to discuss $choose-pivot$ and $partition$.

$choose-pivot$ simply returns the $\textit{index}$ of the value that we will use to pivot around. Basically, we call $partition$ on an array and tell it to pivot around a certain index. So we use choose-pivot to return a value $p$ and then partition returns an index $i$ such that everything before $i$ is smaller than the pivot's value and everything after $i + 1$ is bigger than the pivot's value and $i$ is the index of the pivot we chose:
```C++
choose-pivot(A) {
	// later on, we can randomly choose a pivot
    return A.size() - 1;
}

partition(A, p) {
    int pivot-value = A[p];
    Array small[];
    Array large[];
    for (int i = 0; i < A.size(); i++) {
    	if (i == p) continue;
        if (A[i] < pivot-value) small.push_back(A[i]);
        else large.push_back(A[i]);
    }
    int i = small.size();
    A[0...i-1] = small;
    A[i] = pivot-value;
    A[i+1...n-1] = large;
    return i;
}
```
Now we can describe quick-select. We choose a pivot and partition a given array A by it. Suppose we want the k'th largest element. Then we $\textit{know}$ that the value at $i$ is without a doubt the $ith$ largest element by definition of partition (everything from 0 to $i-1$ is less than the pivot value at index $i$). So if $k = i$, then we're done and we found our element. If we were looking for a bigger element however, we need to look at everything after index $i$ and run quick select on it, updating the value of $k$ accordingly. Otherwise, we simply call quick-select on everything from $0$ to $i-1$:
```C++
quick-select(Array A, int k) {
    p = choose-pivot(A);
    i = partition(A, p);
    if (i == k) {
    	return A[i];
    }
    if (i < k) {
    	return quick-select(A[i+1, ..., n - 1], k - i - 1);
    }
    return quick-select(A[0, ..., i - 1], k);
}
```
There are $(n-1)!$ permutations of how $i$ could be less than $v$ so:
$$
\begin{align*}
T(n)_{avg} &= \frac{1}{n!} \sum_{i=0}^{size(I)}\\
&= O(n) + \frac{1}{n} max(T(i), T(n-i-1))
\end{align*}
$$
On the other hand, if we use randomized pivot, we can assume that, with probability $\frac{1}{n}$, the random pivot has the index $i$:
$$
\begin{align*}
T^{expected}(n)_{avg} &= \frac{1}{n} (\frac{1}{(n-1)!} (n^2) + O(n) + \frac{1}{n} max(T(i), T(n-i-1)))
\end{align*}
$$
Honestly super unsure about the above equation. Anyways, quick-sort works the same way as quick-select:
```C++
quick-sort(Array A) {
    int p = choose-pivot(A);
    int i = partition(A, p);
    quick-sort(A[0...i-1];
    quick-sort(A[i+1...n-1];
}
```
Quick-sort is usually the most efficient algorithm for sorting, running in $O(nlogn)$ with expected time $\Theta(nlogn)$.

$\textit{No comparison based algorithm can sort faster than $nlogn$ runtime.}$ The lower bound on comparison based sorting is $\Omega(n \log n)$.

Proof: In a binary tree of height $h$, there are no more than $2^{h+1}$ leaves. Furthermore, sorting $n$ integers can be done in $n!$ ways. In a decision tree, the number of comparisons is the same as the height $h$. Therefore, there should be $n!$ leaves which is to say:
$$
\begin{align*}
n! &\leq 2^{h+1}\\
\log (n!) &\leq h + 1
\end{align*}
$$
Note that $\log(n!)$ is $\Theta(nlogn)$. So we have that there is a lowerbound on the number of comparisons of $\Omega(n\log n)$.

Non-comparison based sorting, can be fast than $\Omega(n log n)$. We assume that the keys are numbers in base R and have the same number of digits. 

Bucketsort creates $R$ lists. It then looks at the digit at a given index, $d$, for each number in A, and puts the number in list $d$. So if we look at the first number while looking for the least significant digit and it has digit 0 at the end, we put the number in the 0th list. It does this for every number and returns an array that is sorted based on that one digit:
```C++
bucket-sort(Array A, int d) {
    B[0...R-1] is our array of lists;
    for (int value : A) {
    	B[d'th digit of value].push_back(value);
    }
    int i = 0;
    for (list bucket : B) {
    	for (int value : bucket) {
        	A[i] = value;
            i++;
        }
    }
}
```
This takes time $\Theta(n + R)$ with space $\Theta(n)$. It's also a $\textit{stable sorting algorithm}$ because equal items retain their order to each other.

Count Sort tries to avoid the auxiliary space that's wasted. It holds the $\textit{count}$ of numbers that end with a certain digit rather than copying those numbers. Then, it goes through A, and uses the count to find the index to copy into. Pseudocode in the slides.

MSD-radix-sort uses the most significant digit and recursively sorts by count-sorting through each digit until the least significant digit. Overall runtime is still $O(m(n+R))$. Also, what if length is 1, partition falls apart.

LSD-radix-sort is far simpler. We go through each digit starting from least significant until most and run count-sort on it based on that digit. We can do it iteratively like this because ordering moving from right to left doesn't destroy the order. If we did this with MSD, we'd sort by left most, and then once we moved to the right, the order would be based solely on the second-left most:
```C++
LSD-radix-sort(A) {
    for each (int i = R - 1; i >= 0;) {
    	count-sort(A, d);
    }
}
count-sort(Array A, int d) {
    int count[R] = {0,...,0}
    for (int value : A) {
    	count[d'th digit of value]++;
    }
    int index[R] = {};
    index[0] = 0;
    for (int i = 1; i < index.size(); i++) {
    	index[i] = index[i-1] + count[i-1];
    }
    int aux[n] = {};
    for (int value : A) {
    	aux[index[value]] = value;
        index[value]++;
    }
    A = aux;
}
```
### Tutorial
##### Problem 1
This problem was so messed up, it can't be done.

##### Problem 2
Give the best-case, worst-case, best-case expected, worst-case expected of the following function. Shuggle takes $O(n)$ time.
```C++
BOGO(Array A) {
    shuffle(A);
    if (A.isSorted()) {
        return A;
    }
    return BOGO(A);
}
```
Best-case: we get lucky and it's somehow sorted the first time so we have $O(n)$. For the worst case, there are ${n!}$ permutations of an array of size $n$. In the worst case, we go on forever and ever and never hit the permutation that gives us a sorted array. In the best/worst case expected, we go through EACH permutation and don't have a sorted array till the last one giving us worst-case runtime $O(n * n!)$.

##### Problem 3
I guess we can just do linear search in this case?

## Dictionaries and Balanced Search Trees

A dictionary is a collection of items, each of which contains a key and some data, called a key-value pair. Often implemented as binary search trees. But BST's have limitations. BST's can have a worst case height of $O(n)$.

An AVL tree is a BST with a restriction on height balance. AVL Trees have the property that $height(L) - height(r) \in \{-1, 0, 1\}$ where L is the left subtree and R is the right subtree. The height is $\textit{always}$ $\in \Theta(\log n)$.

$\textbf{Insertion}$: Do a normal BST insert. Go upwards from point of insertion and update heights. If we encounter disbalance of $\pm 2$, we have to balance. An insertion will $\textbf{always}$ require at $\textbf{most}$ one rotation. Inserting into an AVL tree is therefore $\Theta(\log n)$. 

Fixing rotations:

![AVL Rotations](fixing-avl.jpg "AVL Rotations")

$\textbf{Deletion:}$ Do a normal BST delete.  We assume BST-delete will give us the parent of the node we deleted. Then, we move upwards, updating the heights. We rotate on the node if there's a disbalance. Also $\Theta(\log n)$:

$$
avl-insert(Node root, Key k, Value v) {
    Node z = BST-insert(root, k, v);
    z.height = 0;
    while (z != null) {
    	setHeightFromChildren(z);
        if (z.isDisbalanced()) {
        	AVL-fix(z);
            break;
        }
        z = parent(z);
    }
}

avl-delete(Node root, Key k) {
    Node z = BST-delete(root, k);
    while (z != null) {
    	setHeightFromChildren(z);
        if(z.isDisbalanced()) {
        	AVL-fix(z);
            // no break, we go up all the way.
        }
        z = parent(z);
    }
}
$$

Overall, AVL trees are really nice for guaranteeing $\Theta(\log n)$ operations.

### Tutorial

##### Problem 1

Consider the problem of sortin an array $A - (a_1, a_2,\ldots, a_n)$ of elements with multiplicity $\frac{n}{k}$. That is, $A$ is made up of $k$ distinct elements ($y_1, y_2, \ldots, y_k)$, where $y_i$ occurs $\frac{n}{k}$ times in $A$. Prove that any algorithm in the comparison model requires $\Omega(n\log k)$ comparisons to sort $A$.

Every algorithm that is comparison-based can be shown in a decision tree where each node is a comparison that we made based on previous comparisons. We showed in class that $\text{(maximum number of nodes}) \geq \text{number of leaves}$. We also know that the number of leaves is given by the permutations of input and the height of the tree is the number of comparisons we make. There are at most $2^{h+1}$ nodes in a decision tree. In this case, the number of permutations is $\frac{n!}{(\frac{n}{k})!(\frac{n}{k})!\cdots(\frac{n}{k})!}$. So we have that:
$$
\begin{align*}
    2^{h+1} &\geq \frac{n!}{(\frac{n}{k})!(\frac{n}{k})!\cdots(\frac{n}{k})!}\\
    &=\geq{n!}{((\frac{n}{k})!)^k}
\end{align*}
$$
Taking the log of both sides we get:
$$
\begin{align*}
    h + 1 &\geq \log(n!) - k\log((\frac{n}{k})!)\\
    &\geq n\log n - k(\frac{n}{k}\log(\frac{n}{k}))\\
    &\geq n\log n - n\log\frac{n}{k}\\
    &\geq n(\log n - \log(\frac{n}{k}))\\
    &\geq n(\log \frac{n}{\frac{n}{k}})\\
    h + 1 &\geq n\log k
\end{align*}
$$
So we have that $h \in \Omega(n\log k)$
##### Problem 2
Suppose that we have an unsorted array $A$ of $n$ non-negative intergers, where $A[i] < n^c$, for some fixed known constant $c$. Describe how to sort $A$ in $O(n)$ time.

I will assume $c \geq 1$. We can use radix-sort which allows us to sort $n$ $m$-digit base R numbers in $\Theta(m(n+R))$. If we look at all the entries in the $A$ as base $R = n$, we get every number in A encoded with $m = c$ digits (since each digit is at most $n$ and we have a maximum bound of $n^c$. This gives runtime of $\Theta(m(n + R)) = \Theta(2cn) = \Theta(n)$.

##### Problem 3
Basically we can implement count-sort but slightly different. We create an array count of size $k$ where count[$i$] stores the number of values that are equal to $i$. Then we go through each number in $A$ and increment count[$i$] as needed. Then we create an array called index of size k + 1 where index[$i$] is the number of elements in A that are less than $i$. So we loop from 1 to $k$, (we set index[0] = 0) and say that $index[$i$] = index[i-1] + count[i-1]$. It takes $O(n)$ time to make the count array and $O(k)$ time to make the index array giving $O(n + k)$ as needed. Querying takings constant time because we simply call $index[b] - index[a]$.

## Other Dictionary Implementations
Suppose we have a linked list implementation of a dictionary. We also seem to know the access probability $P(x_i)$ for a given value in the list $L = \{x_1, x_2, \cdots x_n \}$. Furthermore, we know that the position of $x_i$ is given by $T(x_i)$. We have then, that \textbf{the expected access cost of $x_i$} is $E(L) = \sum_{i=1}^{n} P(x_i) T(x_i)$. Overall all possible sortings, the one that sorts items by non-increasing access-probability minimized the expected access cost. That is, the items that have the highest access probability should be at the front. Why? Because it takes fewer traversals of the linked list to get to those items. If we want to get the very last element, we have to go through the $first\to next\to next\to\cdots\to next$.

Dynamic Ordering is necessary when we don't know the access probabilities. The rule of thumb with these is that a recently accessed item is likely to be accessed again soon. Furthermore, we always insert at the front. There are two heuristics for dynamic ordering:

1. Move-to-Front (MTF): Every time you access an item, move that item to the front of the list.
1. Transpose: Every time you access an item, swap the item forwards.

MTF works best in practice. Furthermore, we can show that MTF is "2-competitive" that is to say:
$$
\begin{align*}
Cost_{MTF} \leq Cost_{optimal} \leq 2 \cdot Cost{MTF}
\end{align*}\vspace{2mm}
$$

Skip Lists are a \textbf{randomized} data structure for a dictionary ADT. It consists of a hierarchy, $S$, of ordered linked lists $S_0, S_1, \cdots, S_h$.

1. Each list $S_i$ contains the special keys $-\infty$ and $+\infty$ . 
1. List $S_0$ contains the key value pairs of $S$ in non-decreasing order
1. Each list is a subsequence of the previous one. $S_0 \supset S_1 \supset \cdots \supset S_h$.
1. The top list, $S_h$ contains only the special keys.

A skip list is considered a two dimension collection of positions. Each list a $\textit{level}$ and each element's height is a $\textit{tower}$. The skip-list itself is a reference to the topleft node. Each node $p$ has a reference to $after(p)$, the next node at the current level and $below(p)$ which is the node directly below $p$. $below(p)$ should return a reference to a node with the same value but in the list at a lower level.

![Skiplist](skiplist.jpg "Skiplist")

Searching in a skip list starts at the top left and goes down one level. We then move forward in the level until the key after us is greater than what we're looking for. We move down one level and repeat until we find it or there's nothing below us. Interesting thing to note: we store every node that we encounter on a stack:
```C++
skip-search(SkipList L, Key k) {
    Node p = L.topLeft();
    Stack P; P.push(p);
    while (below(p) != null) {
    	p = below(p);
        while (after(p).getKey() < k) {
        	p = after(p);
        }
        P.push(p);
    }
    return P;
}
```
![Skiplist search](skiplistsearc.jpg "Skiplist search")

Inserting in a skip list is a randomized algorithm. Toss a coin until it comes up tails. The number of heads, $i$, is the height of the tower of the new key $k$. So we insert into $S_0, \cdots, S_{i}$. If this means the top level is altered, we increase the height of the skip list until $h > i$. We search for $k$ to get a stack $P$ of all of the predecessors in each level of $k$:
```C++
skip-insert(SkipList L, Key k) {
    int i = 0;
    while (coinflip() == heads) {
    	i++;
    }
    while (i > L.height()) L.increaseHeight(i+1);
    int h = L.height();
    Stack P = skip-search(L, k);
    Node p = L.topLeft();
    while (below(p) != null) {
    	p = below(p);
		h--;
        if (h > i) continue;
        while (after(p).getKey < P.top()) {
        	p = after(p);
        }
        p.insertAfter(P.pop());
    }
    return P.size() == 0;
}
```
Deletion from a skiplist is similar to insertion. We search for the key $k$ to be deleted which gives us a stack of all of its predecessors and itself. Then we go through each level, find the predecessor, and delete after(predecessor). Remove any extra lists. Note that deletion happens from the bottom up.
```C++
skip-delete(SkipList L, Key k) {
    Stack P = skip-search(L, k);
    Node p = L.topLeft();
    while (below(p) != null) {
    	p = below(p);
        while (after(p).getKey <= P.top()) {
        	p = after(p);
        }
        P.pop();
    }
    // p is now the k at the lowest level
    p.deleteTower();
    L.flatten();
    return P.size() == 0;
}
```
All of $skip-search$, $skip-insert$, $skip-delete$ are $O(\log n)$ expected time.

### Tutorial

##### Problem 2
$\textbf{Recurrence Relation:}$ A tree $T_h$ is constructed by making $T_{h-1}$ the left subtree and $T_{h-3}$ the right subtree.

$\textbf{Proposition:}$ Let $R(i)$ = if $m_i$ is the minimum number of nodes in an AVL-2 tree of height $i \geq 0$, then $m_i \geq 2^{\frac{i}{3}}$.

$\textbf{Base Case:}$ When $h = 0$, the minimum number of nodes is 1. Thus $R(0)$ holds.

$\textbf{Inductive Hypothesis:}$ Let $R(i)$ hold for all $i$ in $0,\ldots,k$.

$\textbf{Inductive Step:}$ Let $h = k + 1$.

Then we have that to minimize $m_h$, $m_h = m_{h-1} + m_{h-3} + 1$. So we have that:
$$
\begin{align*}
    m_{k+1} &= m_k + m_{k-2} + 1\\
    &\geq 2^{\frac{k}{3}} + 2^{\frac{k-2}{3}}\\
    &= 2^{\frac{k-2}{3}}(2^{\frac{2}{3}} + 1)\\
    &\geq 2^{\frac{k-2}{3}}(1 + 1)\\
    &= 2^{\frac{k+1}{3}}
\end{align*}
$$
As needed.

##### Problem 3
Consider a list $L = \{I_1, I_2, \ldots, I_{n-1}, I_{n}\}$. If we search for $I_{n}$ and then $I_{n-1}$ over and over again $m$ times, we have to pay $O(n)$ for each call giving us total runtime $\Theta(n\cdot m)$. In MTF, these two elements get moved to the front immediately meaning we do not raverse more than one node if we were to make the same searches.

## Midterm Review Session
### Runtime Analysis:
$\textit{JohnSort:}$
1. Verify runs in $O(n)$ time, no matter what.
1. HeapSort runs in $O(n\log n)$ time given a good mood
1. SelectionSort runs in $O(n^2)$ time given a bad mood
1. MSDRadixSort runs in $O(n)$ time given no mood
	1. MSD runs in O(m(n + k)) time actually
$\textit{SajedSort:}$
1. Verify runs in $O(n)$ time, no matter what.
1. SelectionSort runs in $O(n\log n)$ time given a good mood
1. LSDRadixSort runs in $O(n)$ time given a bad mood
	1. LSD runs in O(m(n + k))
1. MergeSort runs in $O(nlogn)$ time given no mood
$\textit{Analysis:}$
1. $\textbf{Best Case:}$
  1. JohnSort in no mood, $O(n) + O(n) = O(n)$
  1. SajedSort in bad mood, $O(n) + O(n) = O(n)$
1. $\textbf{Worst Case:}$
 	1. JohnSort, $O(n) + O(n^2)$ with a bad mood
	1. SajedSort, $O(n) + O(n^2)$ with a good mood

For the $\textbf{average case}$, we have for JohnSort that the average is $\frac{1}{size(A)}$ ((good mood) + (bad mood) + (sum of all other cases)). For JohnSort that means:
$$
\begin{align*}
\frac{1}{n} \left(O(n\log n) + O(n^2) + (n - 2) \cdot O(n) \right)\\
O(\log n) + O(n) + O(n) \in O(n)
\end{align*}
$$
For the $\textbf{average case}$, we have for SajedSort that the average is $\frac{1}{size(A)}$ (final element is max) + (final element is min) + (sum of all other cases). For JohnSort that means:
$$
\begin{align*}
\frac{1}{n} \left(O(n^2) + O(n) + (n - 2) \cdot O(nlogn) \right)\\
O(n) + O(1) + O(nlogn) \in O(nlogn)
\end{align*}
$$

For BenjaminSort, we consider the runtime given all possible inputs:
1. Good Mood: $\frac{1}{2} ( \theta(nlogn) + \Theta(n^2)) \in \Theta(n^2)$
1. Bad Mood: $\frac{1}{2} ( \theta(n) + \Theta(n^2)) \in \Theta(n^2)$
1. No Mood: $\frac{1}{2} ( \theta(n) + \Theta(nlogn)) \in \Theta(nlogn)$

So we look at the worst case, best-case using the above list. The average case is done similar to the previous average case (the answer is going to be $\Theta(nlogn)$:
$$
\begin{align*}
&= \frac{1}{n} \left( \Theta(n^2) + \Theta(n^2) + (n-2) \Theta(nlogn) \right)\\
&\in \Theta(n) + \Theta(n) + \Theta(n log n)
\end{align*}
$$

### $d$-ary Heaps
Let R(h) be the property that a d-ary heap with height h has at minimum $d^h$ nodes and at most $d^{h+1}$ nodes, with $d \geq 1$.
\textbf{Base Case:} Let $T_0$ be a tree with height 0. Then $T_0$ has only one node. That is:
$$
\begin{align*}
d^h \leq 1 \leq d^{h+1}\\
1 \leq 1 \leq d
\end{align*}
$$
We know that $d \geq 1$ so the base case holds.

$\textbf{Inductive Hypothesis:}$ Suppose R(k) holds for all $i$ in $0 \cdot k$ for some $k$.

$\textbf{Inductive Step:}$ Consider $h = k+1$. By our IH, we know that R(k) holds so we have the for a d-ary tree of height $k$:
$$
\begin{equation}
d^k \leq nodes \leq d^{k+1}
\end{equation}
$$
But we know that a tree with height $k +1$ has one extra level. At minimum, that means that $T_{k+1}$ has $k$th level full and ($k+1$)th level has at least one node. This gives a lower bound of $d^{k+1} + 1$. Furthermore, we know that the $(k+1)$th level could be full, meaning that each node in the $k$th level has $d$ children. That is, all $k$ nodes have $d$ children, meaning there could be $d^{k+1}$ leaves. In a tree with $d^{k+1}$ leaves, we must have a tree that has a total of $\sum_{i=0}^{k+1} d^i = \frac{d^{k+2} - 1}{d - 1} \in O(d^{k+2})$  nodes. So we have an upper bound of $d^{k+2}$ nodes. Thus we have that
$$
\begin{equation}
d^{k+1} \leq nodes \leq d^{k+2}
\end{equation}
$$
as needed. Thus R(k+1) holds and induction wins. Finding the height is trivial and we have a $\Theta(\log_d n)$ height.

1. Give indices if stored in array
	1. Don't think this is hard, we just look at $A[di + 1 \cdots d]$ for children and $A[\frac{i-(d-1)}{d}]$ for parent. That's the only difference.
1. Give an insertion algorithm
	1. I think no change. Fix-up as usual.
1. Give a deletion algorithm
	1. Same thing no change.

### String Sorting
Use MSD-sort to avoid having to worry about lengths. We let R = 26. 

### Order Notation
$$
\begin{align*}
2n^7 + 15n^3\log(n) + 2018 &\leq c g(n)\\
&\leq c (2n^7 + 15n^4 + 2018)\\
&\leq c (2n^7 + 15n^4 + n), n_0 = 2018\\
&\leq c (2n^8) \\
&\leq c(n^9)
\end{align*}
$$
So for $c=1, n_0 = 2018$, we have that $f(n) \in O(g(n))$.

$$ 
\begin{align*}
L &= \lim_{n\to\infty} \frac{\frac{n^2}{n + \log n}}{n}\\
&=\lim_{n\to\infty} \frac{n}{n + \log n}\\
&=\lim_{n\to\infty} \frac{1}{1 + \frac{1}{n}}\\
&= 1
\end{align*}
$$
By the limit rule, since $f(n)$ and $g(n)$ grow at similar rates, we have that $f(n) \in\Theta(g(n))$.

We have that $f(n) = n!, g(n) = n^n$. Note that:
$$
\begin{align*}
n! &= n \cdot (n-1) \cdot \cdots \cdot 1\\
n^n &= n \cdot \cdots \cdot n
\end{align*}
$$
Each function has $n$ terms where the $i$th term of $f(n)$ is less than equal to the $i$th term of $g(n)$. We can say then that:
$$
\begin{align*}
L &= \lim_{n\to\infty} \frac{f(n)}{g(n)}\\
&= \lim_{n\to\infty} \frac{n!}{n^n}\\
&= 0
\end{align*}
$$
Thus, by limit rule $f(n) \in o(g(n))$.

It's worth noting that the only difference between Onion notation and big O notation is that onion applies for all $n \geq 0$. We let $g(n) = n$ and $f(n) = n^2$. Then for $c=1, n\geq 0$, $g(n) \in Onion(f(n))$. Easy disproof of Frenette's proposition: let $g(n) = n^2 + 1, f(n) = n^2$. Then it is true that $g(n) \in O(f(n))$. BUT, if we let $n = 0$, we have that:
$$
\begin{align*}
n^2 + 1 \leq c \cdot n^2\\
1 \leq c \cdot 0
\end{align*}
$$
There is no constant $c  > 0$ that makes the inequality valid. Thus we have a disproof.

### Updating Partial Sum:
We use an AVL tree wherein each node has the following fields:
* ${\tt key}$ -- the key of the node;
* ${\tt left}$ -- pointer to the left child;
* ${\tt right}$ -- pointer to the right child;
* ${\tt balance}$ -- balance factor of the node;
* ${\tt parent}$ -- pointer to the parent of the node;
* ${\tt isLeft}$ -- is true if the node is a left child of its parent;
* ${\tt isRight}$ -- is true if the node is a right child of its parent;
* ${\tt numLeft}$ -- holds the number of nodes in the left subtree of the node;
* ${\tt numRight}$ -- holds the number of nodes in the right subtree of the node.
* ${\tt sumChildren}$ -- holds the sum of all children
Note that each key = value.

We add simply by doing a normal BST-insert and then updating the sum of everything from our node up until the parent. If a rotation is required, our sum will need to be updated recursively from top to bottom for each left and right subtree in logarithmic time:
```C++
updateSums(Node root) {
    if (root.hasNoChildren) {
    	return 0;
    }
    int answer =  0;
    if (root.numLeft == 1) {
    	answer += root.left.value;
    } else {
       answer += updateSums(root.left);
    }
    if (root.numRight == 1) {
    	answer += root.right.value;
    } else {
        root.left.sumChildren = updateSums(root.right);
    }
}
```
We update without any issues by changing the value and then updating the sum of everything from our node until the root.

We can get the partial sum by getting the $k$th node and getting its value for sumChildren.

### Union Selection
We create two arrays, $A =\{a_0,\cdots,a_k\}, B=\{b_0,\cdots,b_k\}$ by taking the first $k$ elements of each array. Then we find the median of each array $a_m, b_m$. If $a_m > b_m$, we recursively look at all of $A$ and half of $B$. Similarly, if $a_m \leq b_m$, we recrusively look at half of A and all of B. If the sum of the indices of $a_m, b_m$ is less than k, then we have to look at the top half and adjust $k$ accordingly in our recursive call. If the sum of the indices is greater than k, then we can look in the bottom half without adjusting $k$:
```C++
union-selction(int k, Array A, Array B) {
	if (A.size() == 0) return B.at(k);
    if(B.size() == 0) return A.at(k);
    int midIndexA = middle index of A;
    int midIndexB = middle index of B;
    if (midIndex A + midIndexB < k) {
    	if (A[midIndexA] > B[midIndexB]) {
        	return union-selection(k - midIndexB, A.topHalf, B);
        } else {
        	return union-selection(k - midIndexA, A, B.topHalf);
        }
    } else {
        if (A[midIndexA] > B[midIndexB]) {
        	return union-selection(k - midIndexB, A.bottomHalf, B);
        } else {
        	return union-selection(k - midIndexA, A, B.bottomHalf);
        }
    }
}
```
### Pseudocode Runtime Analysis
Stoogesort: We break up the array into sub arrays of size $\frac{2n}{3}$. This suggests we have $\log_{\frac{3}{2}}$ recurses. However, we do this for three different chunks each time recursively, giving us a total of $3^{\log_{\frac{3}{2}}n}$ total operations. Now, note the change of base formula is:
$$
\begin{equation}
a^{\log_b c} = c^{\log_b a}
\end{equation}
$$
Then,  $3^{\log_{\frac{3}{2}}n} = n^{\log_{\frac{3}{2}}3} $. Thus $T(n) \in O(n^{\log_{\frac{3}{2}}3})$ we could change the base if we wanted to get a slightly different but equivalent formula).

For mystery, we let $i$ be the number of iterations. We note that $k$ is the difference between powers of two. And thus since we increase $j$ by $k$, we have that $j$ is simply a power of two such that $j = i^2$. Then we loop while $i^2 \leq n$. That is, we do a total of $\sqrt[]{n}$ operations. So we have that $T(n) \in O(\sqrt[]{n})$.

The only way for the bonus program to terminate is if $x = 2^i$ for some $i$. If $x$ is odd however, it could take a really long time until $x$ is a power of 2. I don't quite know how to prove complexity for this, or even how to prove that it terminates.

## Dictionaries for special keys
$\textit{The fastest implementation of the dictionary ADT requires $\Theta(\log n)$ time}$. Proof is left as an exercise (until I find it).

$\textbf{Interpolation Search}$ is useful for when we have a sorted array that is uniformly distributed. In binary search, we start from the middle and use a divide and conquer strategy to find a key, $k$. In interpolation search, we assume the values are distributed so we use the first and last values to find the weigted average of where we expected our value $k$ to be roughly. However, there is worst case performance in linear time when the keys are not uniformly distributed. For example, $\{1,2,3, 1000\}$ will cause linear time search for 1. Instead of starting in the middle such that $m = n/2$. We start with a weighted average such that $m = l + \frac{k-A[l]}{(A[r]-A[l])} (r - l)$:
```C++
interpolation-search(Array A, Key k) {
    int l = 0;
    int r = A.size() - 1;
    // while k is between the left and right values
    // and the left and right values are not the same
    while (A[r] != A[l] && k >= A[l] && k <= A[r]) {
    	int m = l + ((k-A[l]) / (A[r] - A[l])) * (r - l);
        if (A[m] < k) l = m+1;
        else if (k < A[m]) r = m+1;
        else return m;
    }
    if (k = A[l]) return l;
    else return -1;
}
```
### Tutorial
##### Problem 1
We suppose that $A[i] = ai + b$ for some real numbers $a,b$. The first index that interpolation search will look at (when $l=0$, $r=n-1$) is:
$$
\begin{align*}
    i &= l + \frac{k-A[l]}{A[r] - A[l]} (r - l)\\
    i &= \frac{k-b}{(a (n-1) + b) - (b)} (n - 1)\\
    i &= \frac{k-b}{a}
\end{align*}
$$
Now, consider the case where $k$ is indeed stored in the array. Then we have that $k = at + b$ for some index $t$. Then we have that:
$$
\begin{align*}
    i &= \frac{k-b}{a}\\
    &= \frac{at+b - b}{a}\\
    &= \frac{at}{a}\\
    &= t
\end{align*}
$$
meaning that we find $k$ within out first search(!).

If $k$ isn't in our array then we say that $k = az + b$ for some index $b$ that is between $b-1$ and $b+1$ such that $0 < b < 1$. That is to say that if $k$ was in the array, it would be between two numbers (like half an index). So we get that $i = z$. Then when we check $A[z]$ and see that it's not $k$, we recurse again on $z + 1$ and find that $A[z + 1] > k$ and we return a false. (????).

##### Problem 3
We put all the keys in the bottom level. Then we promote every other elements to the top (half the elements) ensuring that $-\infty$ and $+\infty$ are promoted. We repeat this until only $-\infty$ and $+\infty$ are left. Generating the first level costs us $O(n)$. The second requires $O(n)$ as well. The third requires $O(\frac{n}{2}$. We see that in general, to build $S_i$ it takes time $O(\frac{n}{2^{i-1}}$. The total runtime is then given by:
$$
\begin{align*}
    T(n) &= n + \sum_{i=1}^{\log n} \frac{n}{2^{i-1}}\\
    &= n + n \sum_{i=1}^{\log n} \frac{1}{2^{i-1}}\\
    &\in O(n)
\end{align*}
$$

## Dictionaries via Hashing
### Hashing Introduction
If we have $M$ keys, such that every $k \leq M$ that we want to store in a dictionary, we can use an array $A$ of size $M$ that stores each key-value pair where:

1. search(k): $\Theta(1)$
1. insert(k, v): $\Theta(1)$
1. delete(k): $\Theta(1)$

And it takes $\Theta(M)$ space. Similar to $\textit{counting sort}$.

Issues: Keys are not always integers. And what if $n$, the number of keys, is significanty less than $M$? Then we have a lot of empty, unused space in the array.

$\textbf{Hashing Idea:}$ Map the keys to a small range of integers and THEN use the direct addressing above. Details:
1. Assumption that all keys come from some \textit{universe} $U$ (usually $U = N$)
1. \textbf{Hash function} $h : U \to \{0,1,\ldots, M-1\}$. (e.g. $h(k) = k \mod M$ (called Euclidean division) -- this is just an example of a simple (but weak) hashing function)
1. Store dictionary in a \textit{hash table}: Array $T$ of size $M$. Key $K$ is stored in $T[h(k)]$. (basically we need to create an array where each key maps to an index based on a hash function.

Hash functions are $\textbf{not}$ $\textit{injective}$ -- collisions are more or less expected in real-life applications.

$\textbf{Euclidean division:}$ Given $n, M > 0$ integers, there are unique $q,r \in \{0, \ldots, M-1\}$ such that $n = qM + r$ where $r = n\mod M$.

The issue with using Euclidean division is that we're going to have collisions. For example $18 \mod 11$ and $7\mod 11$ are both $7$ so we'd have two different keys that need to be stored at the same value $h(k)$.

Suppose we choose $n$ values at random in $\{0,\ldots,M-1\}$.

Probability of no collision is (here we're simply finding the number of ways that we can pick n items from M such that order matters and there are no duplicates, and then comparing that to the total number of ways of selecting \textbf{any} $n$ items from $M$ (including collisions), order matters):
$$
\begin{align*}
    &= \frac{M \cdot (M-1)\cdot(M-2)\cdot\ldots\cdot(M-(n-1))}{M^n}\\
    &= {(1-\frac{0}{M})\cdot(1-\frac{1}{M})\cdot\ldots\cdot(1-\frac{n-1}{M})}
\end{align*}
$$
So the probability of collision is:
$$
\begin{align*}
    f(n, M) &= 1 - {(1-\frac{0}{M})\cdot\ldots\cdot(1-\frac{n-1}{M})} \\
    &= 1 - \frac{_M P_n}{M^n}
\end{align*}
$$
$\textbf{Example:}$ The birthday paradox. $f(22, 365) < 0.5)$ and $f(23, 365) > 0.5$.

Dealing with collisions: 
    * Allow multiple items at each index (called chaining)
    * Allow each item to go into multiple places (open addressing).
        * Allow many other location (probe sequences)
        * One other (cuckoo hashing)
We evaluate these strategies based on the cost of the three operations search, insert, delete. The $\textit{load factor}$ $\alpha = \frac{n}{M}$ (how much of the array is used up). We keep the load factor small by $\textit{rehashing}$ when needed:

1. Keep track of $n$ and $M$ throughout each operation
1. When $\alpha$ gets too big, double the hash table, and reinsert the elements using the (new) hash function values.
    1. Rehashing costs $\Theta(M+n)$ but is so rare that we ignore and amortize it over operations
    1. $\textbf{We also rehash when $\alpha$ is too small, in order to maintain $\Theta(n) space$}$.

### Separate Chaining
Each entry in the table is a $\textit{bucket}$ with multiple key value pairs. Technically we could have a dictionary within a dictionary (i.e. a hash table within a hash table). BUT, the easiest way to do this is just an unsorted linked list in each bucket. This is called #\textbf{separate chaining}#. A look at the operations now:

1. search(k): Looks for $k$ in the list at index $h(k)$
1. insert(k, v): $\textit{Add (k,v) to the front of the linked list at index $h(k)$}$
1. delete(k): Search and then delete from the linked list (does this imply we always have a linked list? even for single nodes? Answer: Yes it does. This could be pretty bad if we add nodes that have the same value $h(k)$ and we want the \textit{last} element in the linked list. (Remember, it's unsorted)

$\textbf{Uniform Hashing Assumption:}$ Each hash function value is equally likely. This means we should expect the average bucket size $= \alpha = \frac{n}{M}$. New analysis of operations:
1. search: $\Theta(1 + \alpha)$ average, $\Theta(n)$ worst case
1. insert: constant (we always insert at front of list so no traversal)
1. delete: $\Theta(1 + \alpha)$ average, $\Theta(n)$ worst case
1. space: $\Theta(M + n) = \Theta(\frac{n}{\alpha} + n$

Uniform hashing: if we hash keys $k_i,\ldots,k_n \to \{0,\ldots,M-1\}$ then:
$$
\begin{align*}
    fn \begin{cases}
    i = 1,\ldots,n\\
    j = 0,\ldots,M-1
\end{cases}
\end{align*}
$$
Probability($h(k_i) = j) = \frac{1}{M}$

Under uniform hashing, the average length of a bucket is $\alpha = \frac{n}{M}$. Proof:

Define $X_{ij} = \begin{cases}
    1 \text{ if $h(k_i) = j$}\\
    0 \text{ otherwise}
\end{cases}$. (For $i=1,\ldots,n$, $j=0,\ldots,M-1$).

Then length($B_j$ = $\sum_{i=1}^n X_{ij}$ (where $B_j$ is the $j$'th bucket. So we have that:
$$
\begin{align*}
    E(length(b_j)) &= E(\sum_{i=1}^n X_{ij})\\
    &= \sum_{i=1}^n E(X_{ij})\\
    &= \sum_{i=1}^n \frac{1}{M}\\
    &= \frac{n}{M}\\
    &= \alpha
\end{align*}
$$
So we need to maintain the load factor $\alpha \in \Theta(1)$, so that the average cost is $O(1)$ in order have that our average costs are $O(1+1)=O(1)$ and space is $\Theta(\frac{n}{1} + n) = \Theta(n)$. How do we maintain this? $\textit{We rehash whenever $n < c_1 M$ or $n > c_2 M$ for some constants $c_1, c_2$ with $0 < c_1 < c_2$}$.

### Probe Sequences
So we only allow one item per index but each item can go in multiple locations (we have more indexes). So searching and inserting follow a $\textit{probe sequence}$ wherein we try all possible locations a key $k$ until an empty spot is found. As in, we check $h(k,0), h(k,1), h(k,2), \ldots$.

Deleting is weird. We can't quite leave an empty spot because then the searching could end premaurely. You don't wanna shift the elements because it takes too much time. So instead, simply label the index as a deleted empty spot. So now, when we're searching and encounter an empty spot, we continue if its marked as previously deleted.

This is called open addressing and the simplest way to do so is $\textit{linear probing:}$ $h(k, i) = (h(k) + i) \mod M$, for some hash function $h$. The way it works:
1. When you insert and encounter a filled spot, you simply add 1 to the value and mod it (you index forward cyclically)
1. When you delete, you mark the index, $i$, deleted. Now, when we need to search at $i$, we can continue moving forward. Normally, if it's an empty spot, we've exhausted all possible values of $h(k, i)$ so the item doesn't exist. What I mean is that searching essentially traces through insertion and checks that IF the value was inserted it should be in the indexes I'm looking for.

Pseudocode:
```C++
probe-sequence-insert(HashTable T, Pair (key, value)) {
    for (j = 0; j < M; j++) {
        if (T[h(k,j)].isDeletedOrEmpty()) {
            T[h(key,j)] = (key, value);
            return 0; // success
        }
    }
    return 1; // error
}
probe-sequence-search(HashTable T, Key) {
    for (j = 0; j < M; j++) {
        if (T[h(Key,j)].isEmpty()) {
            return "not found"; // fail
        }
        else if (T[h(Key,j)] == Key) {
            return T[h(Key,j)];
        }
        else if (T[h(Key,j)].isDeleted()) {
            continue; //ignore deleted
        }
    }
    return 1; // error
}
```
Some hashing methods require two hash functions $h_1, h_2$. We need these functions to be independent such that the probability $Probability(h_1(k) = i)$ and $Probability(h_2(k) = k)$ are independent probabilities. Using modular hash functions makes it difficult (2 is a multiple of 4 for example). So we use a $\textbf{multiplicative method}$ for the second hash function such that:
$$
\begin{align}
    h(k) = \lfloor M(k A - \lfloor k A \rfloor) \rfloor
\end{align}
$$
1. $A$ is some decimal number such that $0 < A < 1$.
1. $h(k)$ computes the decimal part of $k A$
1. Multiply with $M$ and round down

We multiply the key, $k$, by some decimal number $A$. Then we get the decimal part of the product, multiply it by $M$.
The golden ratio: $A = \phi = \frac{\sqrt{5} - 1}{2} \approx 0.618$.

If we have two hash functions $h_1,h_2$ such that $h_2(k) \not = 0$ and $h_2(k)$ is relative prime (what?) with table size $M$ for all keys $k$. That is, we choose $M$ prime to minimize collisions.

We use $\textit{double hashing}$ such that:
$$
\begin{equation*}
    h(k,i) = h_1(k) + i \cdot h_2(k) \mod M
\end{equation*}
$$
Example of things going wrong without $M$ prime:

Suppose $M=16$, we have key $k$ such that $h_1(k) = 3, h_2(k) = 6$. The following cycle (starting at 3 with $i = 0$) shows how we end up cycling through the same 8 numbers M is not prime.

![Hashing cycle](hashing.jpg "Hashing Cycle")

Length of the cycle = 8 = $\frac{16}{gcd(6,16)}$ where $6 = h_2(k), 16 = M$.
What's the problem with this? We get collisions eventually (after 8 cycles).

### Cuckoo hashing
We still use two independent hash functions $h_1, h_2$. However, an item with key $k$ can $\textit{only}$ be in $T[h_1(k)]$ or $T[h_1(k)]$:

1. Search and Delete take constant time (there's only two places to look)
1. Insert puts a new item into $T[h_1(k)]$. If this is occupied, then we "kick out" the other item $i$ which we then attempt to re-insert into its alterate position $T[h_2(i)]$. This could lead to an undless loop of kicking out.

If we fail to insert, we rehash with larger M and new hash functions. Insert is slow but it is about contant time with a small enough load factor. 

### Choosing a good hash function
Our goal is to satisfy the uniform hashing assumption. It's very hard to do this in practice but we can get the best performance by chooding a hash function that:
1. is unrelated to possible patterns in the data
1. depends on all parts of the key

Suppose we hash strings $S = \{S_0, S_1, \ldots, S_n\}$. $S_i$ are characters.
$$
\begin{align*}
    T &= S_0 + 2^{64} S_1 + 2^{2(64)}S_2, \ldots, 2^{n(64)} S_n\\
    h(s) &= T \mod 2^{64}, M = 2^{64}\\
    &= s_0
\end{align*}
$$
The issue is that we're really only looking at one character of the string!
Second attempt:
$$
\begin{align*}
    T &= S_0 + 2^{64} S_1 + 2^{2(64)}S_2, \ldots, 2^{n(64)} S_n\\
    h(s) &= T \mod (2^{64} 1), M = 2^{64} - 1\\
    &= (S_0 + S_1 + \ldots + S_n) \mod (2^{64} - 1)
\end{align*}
$$
But now we treat all characters are evenly. That is, $h(are) = h(era)$ since we simply look at the sum of the characters not order.

We have two basic methods for integer keys:
1. Modular method. Choose $h(k) = k \mod M$ with M prime.
1. Multiplicative method: Choose $h(k) = \lfloor M(k A - \lfloor k A \rfloor) \rfloor$

##### Universal Hashing
Every hash function must fail for $\textit{some}$ sequence of inputs. The solution is randomization; when we initialize or rehash, we choose a prime number $p > M$ $\textbf{and}$ random numbers $a, b \in \{0,\ldots, p-1\}, a\not= 0$.

For our hash function, we now choose $h(k) = ((ak + b) \mod p) \mod M$. Why does this work?

Also, this lets us say that the probability of a collision is at most $\frac{1}{M}$.

##### Multi-dimensional data
The standard approach when given multi-dimensional data (such as string) is to flatten the string, $w$, to integer $f(w)$:
$$
\begin{align*}
    A\cdot P\cdot P\cdot L\cdot E &\to (65, 80,80,76,69)
    &\to 65R^4 + 80R^3 +80R^2 + 76R^1 + 69R^0
\end{align*}
$$
For some radix. Ideally we pick $R$ prime and $M$ prime not equal.
##### Hashing vs. Balanced Search Trees
Advantages for BST:
* $O(\log n)$ worst case
* No assumptions of input
* We know the exact amount of space used
* Never have to rebuild the entire structure
* supports ordered dictionary operations (rank, select k largest)
Advantages for hashing:
* O(1) operations (under uniform hashing and small $\alpha$
* We can choose ourselves the point at which we want to "rehash"
* With Cuckoo hashing, it's $O(1)$ worst-case for search and delete.

## Final Exam
### Data Structures
$\textbf{HEAPS}$ have the order property where the parent is bigger/smaller than both children. They have the structural property that they're balanced and left aligned. Turning an array into a heap requires heapify which runs in $\log n$ time.

$\textbf{AVL-TREES}$ with $\Theta(n)$ nodes have a height of $\Theta(\log n)$. AVL-Trees with height $\Theta(h)$ have minimum number of nodes $\Theta(2^{h-1} + 1)$ and maximum number of nodes $\Theta(2^h - 1)$. Insert, Delete, Search \textbf{all} run in $\Theta(\log n)$ time. 

$\textbf{SKIPLISTS}$ have expected space usage $O(n)$ with expected height $O(\log n)$. Search, Insert, and Delete are all $O(\log n)$ expected time BUT can be far worse in the absolute worst case.

All $\textbf{TRIES}$ (compressed or otherwise) insertion and deletion have time complexity of $\Theta(\vert x \vert)$ where $x$ is the length of the binary string. Worst case space complexity is $O(\vert \Sigma \vert \cdot \vert n \vert)$ where $\vert n \vert$ denotes the total size of the words.

$\textbf{HASHTABLES}$ can be implemented using Cuckoo Hashing, Separate Chaining, Open Addressing, and Universal Hashing. With Cuckoo hashing you consider two independent hash functions. Separate chaining uses linked lists. Open addressing allows values to be put in locations they're not normally. Universal hashing is a randomized algorithm. The goal in every case is to have constant time insert, delete, search. We use the load factor $\alpha = n / M$ to see how full the hashtable is.

$\textbf{QUADTREES}$ have a lot of space taken up. We denote $\beta$ as the spread factor where we take the length of $R$ proportionate to the minimum distance between points. Specifically, QuadTrees have height $\log(\beta)$. Building the initial tree takes time $O(nh)$. Range search takes worst case $O(nh)$.

$\textbf{KD-TREES}$ split based on the median roughly evenly. The height is given by $\log n$ and building a kd-tree takes time $O(n\log n)$. Range search for a kd-tree takes time $O(s + \sqrt{n})$ where $s$ is the number of keys found and $n$ is the total number of keys. Storage and build time is always $O(n)$ and $O(n\log n)$ resepctively.

$\textbf{RANGE TREES}$ use a primary tree sorted by $X$ where each node has an auxiliary tree including its children that is sorted by $Y$. Runtime: $O(\log^2 n + s)$, space: $O(n\log n)$.
### String Matching Algorithms
$\textbf{RABIN KARP}$ improves upon the guess and check system by calculating the hash of the pattern. Then for each guess, it compares the hash before checking.

$\textbf{BOYER MOORE}$ searches starting from the last character of the pattern using the last-occurrence function. If we have a mismatch we shift to the right by the max of (1, pattern index - last occurrence). We do this until our shift causes us to go past the end.

$\textbf{SUFFIX TREES}$ are compressed tries that contain every suffix for a given string $T$. Building this takes time $O(n^2)$ (and possibly $O(n)$). To search for a pattern $P$ we follow through the trie. If we reach a leaf, we still have to check against it in $O(\vert P \vert)$ time.
### Compression
$\textbf{HUFFMAN CODING}$ uses a static dictionary based on the frequencies of each character. The more frequently appearing characters are higher up the huffman tree (so they take fewer bits to encode).

$\textbf{RUN LENGTH ENCODING}$ uses long runs of binary digits. The first number in a compressed text indicates the first number of the actual string. We take $k$ as the number of zeroes. Then we take a look at the next $k+1$ digits following the last 0 to find out how many times the digit is repeated.

$\textbf{LEMPEL ZIV WELCH}$ uses an adaptive dictionary where each useful subtring is added to the dictionary. The result is that the next time we encounter that substring we can use one code to represent it.

$\textbf{MOVE TO FRONT TRANSFORM}$ works by taking a dictionary of letters in a fixed order. As we look through the string, we output the index of the letter from the dictionary. Furthermore, that letter gets moved to the front. So next time, it's at 0.

$\textbf{BURROWS WHEELER TRANSFORM}$ finds all cyclic shifts of a string and sorts them. The coded text is the last character of each string. Decoding requires you to take the coded text and the index of each letter. Then, we sort the text and write the index below it. We start at \$ and follow the index.